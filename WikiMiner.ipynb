{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3F5Ob9wA2POr",
        "outputId": "8ca23006-df3b-4a68-b6e4-f88ec2f48d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Collecting simalign\n",
            "  Downloading simalign-0.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simalign-0.4-py3-none-any.whl (8.1 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, simalign\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 simalign-0.4\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2025.2)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (8.2.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2025.1.31)\n",
            "Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: morfessor, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.92 morfessor-2.0.6 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1\n",
            "Collecting xx-ent-wiki-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-3.8.0/xx_ent_wiki_sm-3.8.0-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xx-ent-wiki-sm\n",
            "Successfully installed xx-ent-wiki-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('xx_ent_wiki_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "!pip install requests beautifulsoup4 nltk pandas tqdm scikit-learn transformers torch spacy simalign pandas numpy matplotlib\n",
        "\n",
        "!pip install indic-nlp-library\n",
        "\n",
        "!python -m spacy download xx_ent_wiki_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# FULL SCRIPT  – single‑pass crawl + multi‑threshold export\n",
        "###############################################################################\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import spacy\n",
        "import logging\n",
        "import numpy as np\n",
        "from simalign import SentenceAligner\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "from indicnlp import common\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ------------------------------------------------------------------ #\n",
        "#  Polite, retry‑aware HTTP session for the Wikimedia API\n",
        "# ------------------------------------------------------------------ #\n",
        "session = requests.Session()\n",
        "session.headers.update({\n",
        "    \"User-Agent\": \"WikiMiner/1.0 (eaa1.17037505@gmail.com)\"  # <-- put real email\n",
        "})\n",
        "retry = Retry(\n",
        "    total=5,                # up to 5 retries\n",
        "    backoff_factor=1.5,     # sleep: 1.5s, 3s, 4.5s, ...\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
        "session.mount(\"http://\",  HTTPAdapter(max_retries=retry))\n",
        "# ------------------------------------------------------------------ #\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "###############################################################################\n",
        "#                              CONFIGURATION\n",
        "###############################################################################\n",
        "\n",
        "INDIC_LANGUAGES = [\n",
        "    'as', 'bn', 'gu', 'hi', 'kn', 'ml', 'mr', 'or', 'pa', 'ta', 'te',\n",
        "]\n",
        "\n",
        "LANG_CODE_MAP = {\n",
        "    'as': 'Assamese', 'bn': 'Bengali', 'gu': 'Gujarati', 'hi': 'Hindi',\n",
        "    'kn': 'Kannada',  'ml': 'Malayalam', 'mr': 'Marathi', 'or': 'Oriya',\n",
        "    'pa': 'Punjabi',  'ta': 'Tamil',     'te': 'Telugu',\n",
        "}\n",
        "\n",
        "# Define categories to crawl in English, which (hopefully) contain articles relevant\n",
        "# to each language. You can customize or expand these categories as needed.\n",
        "language_categories = {\n",
        "    'as': [\n",
        "        \"Articles containing Assamese-language text\",\n",
        "        \"Assamese-language actors\",\n",
        "        \"People from Assam\",\n",
        "        \"Assam Valley Literary Award\",\n",
        "        \"Assamese singers\",\n",
        "        \"Novels in Assamese\",\n",
        "        \"CS1 Assamese-language sources (as)\",\n",
        "        \"Assamese-language film stubs\",\n",
        "        \"Assamese-language mass media\",\n",
        "        \"Assamese people\",\n",
        "        \"Assamese nationalism\",\n",
        "        # Newly added categories for Assamese\n",
        "        \"Assamese literature\",\n",
        "        \"Cinema of Assam\",\n",
        "        \"Music of Assam\",\n",
        "        \"Festivals in Assam\",\n",
        "        \"Assamese cuisine\",\n",
        "        \"Assamese folklore & mythology\",\n",
        "        \"Dances and theatre of Assam\",\n",
        "        \"Assamese-language media\",\n",
        "        \"Notable Assamese people\",\n",
        "        \"Awards and honors in Assam\",\n",
        "    ],\n",
        "    'bn': [\n",
        "        \"Bengali-language films\",\n",
        "        \"Bengali people\",\n",
        "        \"Bengali-language writers\",\n",
        "        \"Bengali words and phrases\",\n",
        "        \"Bengali language\",\n",
        "        \"Articles containing Bengali-language text\",\n",
        "        \"Bengali poetry in English translation\",\n",
        "        \"Songs in Bengali\",\n",
        "        \"Bengali nationalism\",\n",
        "        \"Bengali-language surnames\",\n",
        "        # Newly added categories for Bengali\n",
        "        \"Bengali literature\",\n",
        "        \"Bengali cinema\",\n",
        "        \"Bengali music and songs\",\n",
        "        \"Festivals of Bengal\",\n",
        "        \"Bengali cuisine\",\n",
        "        \"Bengali mythology & folk tales\",\n",
        "        \"Bengali theatre and dance\",\n",
        "        \"Notable Bengali people\",\n",
        "        \"Literary awards in Bengali\",\n",
        "        \"Bengali media\",\n",
        "    ],\n",
        "    'gu': [\n",
        "        \"Gujarati people\",\n",
        "        \"Gujarati writers\",\n",
        "        \"Articles containing Gujarati-language text\",\n",
        "        \"Gujarati literature\",\n",
        "        \"Gujarati-language films\",\n",
        "        \"Gujarati-language books\",\n",
        "        \"Gujarati-language writers\",\n",
        "        \"Gujarati language\",\n",
        "        \"Articles with Gujarati-language sources (gu)\",\n",
        "        \"Gujarati-language songs\",\n",
        "        # Newly added categories for Gujarati\n",
        "        \"Gujarati cinema\",\n",
        "        \"Gujarati music\",\n",
        "        \"Festivals in Gujarat\",\n",
        "        \"Gujarati cuisine\",\n",
        "        \"Gujarati folklore & mythology\",\n",
        "        \"Gujarati theatre (Bhavai)\",\n",
        "        \"Gujarati media\",\n",
        "        \"Notable Gujarati people\",\n",
        "        \"Cultural institutions & awards\",\n",
        "    ],\n",
        "    'hi': [\n",
        "        \"Hindi\",\n",
        "        \"Hindi-language films\",\n",
        "        \"Actors in Hindi cinema\",\n",
        "        \"Hindi-language literature\",\n",
        "        \"Hindi-language film directors\",\n",
        "        \"Hindi-language writers\",\n",
        "        \"Articles containing Hindi-language text\",\n",
        "        \"Hindi theatre\",\n",
        "        \"Hindi-language poets\",\n",
        "        # Newly added categories for Hindi\n",
        "        \"Hindi literature\",\n",
        "        \"Hindi cinema (Bollywood)\",\n",
        "        \"Hindi music and songs\",\n",
        "        \"Indian festivals (North India)\",\n",
        "        \"Hindu epics and mythology\",\n",
        "        \"Hindi media\",\n",
        "        \"Notable Hindi authors and poets\",\n",
        "        \"Hindi theatre and folk arts\",\n",
        "        \"National awards (Hindi context)\",\n",
        "        \"Historical events (Hindi belt)\",\n",
        "    ],\n",
        "    'kn': [\n",
        "        \"Kannada-language films\",\n",
        "        \"Kannada poets\",\n",
        "        \"Kannada playback singers\",\n",
        "        \"Kannada film directors\",\n",
        "        \"Kannada literature\",\n",
        "        \"Kannada cinema\",\n",
        "        \"Kannada grammar\",\n",
        "        \"Kannada-language writers\",\n",
        "        \"Articles containing Kannada-language text\",\n",
        "        \"Kannada people\",\n",
        "        # Newly added categories for Kannada\n",
        "        \"Kannada music\",\n",
        "        \"Festivals of Karnataka\",\n",
        "        \"Kannada cuisine\",\n",
        "        \"Kannada folklore & mythology\",\n",
        "        \"Kannada theater and dance\",\n",
        "        \"Notable Kannada people\",\n",
        "        \"Awards in Kannada\",\n",
        "        \"Kannada media\",\n",
        "    ],\n",
        "    'ml': [\n",
        "        \"Malayalam-language films\",\n",
        "        \"Malayalam cinema\",\n",
        "        \"Articles containing Malayalam-language text\",\n",
        "        \"Malayalam poets\",\n",
        "        \"Malayalam-language writers\",\n",
        "        \"Malayalam grammar\",\n",
        "        \"Malayalam-language newspapers\",\n",
        "        \"Malayalam language\",\n",
        "        \"Malayalam-language novelists\",\n",
        "        # Newly added categories for Malayalam\n",
        "        \"Malayalam literature\",\n",
        "        \"Malayalam music\",\n",
        "        \"Festivals of Kerala\",\n",
        "        \"Malayalam cuisine\",\n",
        "        \"Kerala folklore & mythology\",\n",
        "        \"Malayalam theatre and dance\",\n",
        "        \"Notable Malayalam people\",\n",
        "        \"Malayalam media\",\n",
        "        \"Kerala history events\",\n",
        "    ],\n",
        "    'mr': [\n",
        "        \"Marathi language\",\n",
        "        \"Marathi people\",\n",
        "        \"Marathi-language films\",\n",
        "        \"Actors in Marathi theatre\",\n",
        "        \"Marathi-language writers\",\n",
        "        \"Articles containing Marathi-language text\",\n",
        "        \"Marathi terms\",\n",
        "        \"Marathi-language literature\",\n",
        "        \"Marathi-language poets\",\n",
        "        # Newly added categories for Marathi\n",
        "        \"Marathi literature\",\n",
        "        \"Marathi cinema\",\n",
        "        \"Marathi music\",\n",
        "        \"Festivals in Maharashtra\",\n",
        "        \"Marathi cuisine\",\n",
        "        \"Marathi mythology & Bhakti saints\",\n",
        "        \"Marathi theatre (Sangeet Natak)\",\n",
        "        \"Notable Marathi people\",\n",
        "        \"Marathi media\",\n",
        "        \"Awards in Marathi\",\n",
        "    ],\n",
        "    'or': [\n",
        "        \"Odia language\",\n",
        "        \"Articles containing Odia-language text\",\n",
        "        \"Odia-language writers\",\n",
        "        \"Odia cuisine\",\n",
        "        \"Actors in Odia cinema\",\n",
        "        \"Odia literature\",\n",
        "        \"Odia people\",\n",
        "        \"Odia-language films\",\n",
        "        \"Odia-language actors\",\n",
        "        \"Odia Hindu saints\",\n",
        "        # Newly added categories for Odia\n",
        "        \"Odia music\",\n",
        "        \"Festivals of Odisha\",\n",
        "        \"Odisha folklore & mythology\",\n",
        "        \"Odissi dance and arts\",\n",
        "        \"Notable Odia people\",\n",
        "        \"Odia media\",\n",
        "        \"Odisha awards and organizations\",\n",
        "    ],\n",
        "    'pa': [\n",
        "        \"Punjabi festivals\",\n",
        "        \"Punjabi language\",\n",
        "        \"Punjabi people\",\n",
        "        \"Punjabi dialects\",\n",
        "        \"Punjabi words and phrases\",\n",
        "        \"Punjabi music\",\n",
        "        \"Punjabi-language films\",\n",
        "        \"Punjabi-language writers\",\n",
        "        \"Punjabi-language singers\",\n",
        "        \"Articles containing Punjabi-language text\",\n",
        "        \"Punjabi-language surnames\",\n",
        "        # Newly added categories for Punjabi\n",
        "        \"Punjabi literature\",\n",
        "        \"Punjabi cinema\",\n",
        "        \"Festivals of Punjab\",\n",
        "        \"Punjabi cuisine\",\n",
        "        \"Sikh mythology and history\",\n",
        "        \"Punjabi folk culture\",\n",
        "        \"Notable Punjabi people\",\n",
        "        \"Punjabi media\",\n",
        "        \"Punjab region history events\",\n",
        "    ],\n",
        "    'ta': [\n",
        "        \"Tamil\",\n",
        "        \"Tamil people\",\n",
        "        \"Tamil-language films\",\n",
        "        \"Tamil film directors\",\n",
        "        \"Tamil diaspora\",\n",
        "        \"Tamil film producers\",\n",
        "        \"Tamil writers\",\n",
        "        \"Articles containing Tamil-language text\",\n",
        "        \"Tamil architecture\",\n",
        "        \"People from Tamil Nadu\",\n",
        "        \"Tamil scholars\",\n",
        "        # Newly added categories for Tamil\n",
        "        \"Tamil literature\",\n",
        "        \"Tamil cinema\",\n",
        "        \"Tamil music and songs\",\n",
        "        \"Tamil festivals\",\n",
        "        \"Tamil cuisine\",\n",
        "        \"Tamil mythology and religion\",\n",
        "        \"Tamil dynasties and history\",\n",
        "        \"Notable Tamil people\",\n",
        "        \"Tamil media\",\n",
        "        \"Tamil theatre and arts\",\n",
        "    ],\n",
        "    'te': [\n",
        "        \"Telugu people\",\n",
        "        \"Telugu-language films\",\n",
        "        \"Telugu actors\",\n",
        "        \"Telugu cinema\",\n",
        "        \"Telugu film directors\",\n",
        "        \"Telugu language\",\n",
        "        \"Articles containing Telugu-language text\",\n",
        "        \"Telugu writers\",\n",
        "        \"Telugu poets\",\n",
        "        \"Telugu names\",\n",
        "        # Newly added categories for Telugu\n",
        "        \"Telugu literature\",\n",
        "        \"Telugu music\",\n",
        "        \"Festivals in Andhra/Telangana\",\n",
        "        \"Telugu cuisine\",\n",
        "        \"Telugu mythology & religion\",\n",
        "        \"Telugu theater\",\n",
        "        \"Notable Telugu people\",\n",
        "        \"Telugu media\",\n",
        "        \"Awards in Telugu\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Path to the root directory of the Indic NLP resources\n",
        "# Make sure to point this to your local \"indic_nlp_resources\" folder\n",
        "INDIC_RESOURCES_PATH = 'D:/indic_nlp_resources-master/indic_nlp_resources-master'\n",
        "common.set_resources_path(INDIC_RESOURCES_PATH)\n",
        "# /home/user/112101054_Yash/indic_nlp_resources-master  -- on ubuntu\n",
        "\n",
        "SIM_THRESHOLDS   = [0.6, 0.7, 0.8]            # τ  – cosine similarity\n",
        "ALIGN_THRESHOLDS = [0.4, 0.5, 0.6, 0.7, 0.8]  # α  – word‑alignment ratio\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#                              LOAD MODELS\n",
        "###############################################################################\n",
        "\n",
        "logging.info(\"Loading multilingual transformer model (LaBSE)...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/LaBSE')\n",
        "model     = AutoModel.from_pretrained('sentence-transformers/LaBSE')\n",
        "model.eval()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "logging.info(f\"Transformer model loaded on device: {device}\")\n",
        "\n",
        "logging.info(\"Loading SpaCy models...\")\n",
        "try:\n",
        "    nlp_en = spacy.load('en_core_web_sm')\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error loading SpaCy English model: {e}\")\n",
        "\n",
        "if \"sentencizer\" not in nlp_en.pipe_names:\n",
        "    logging.info(\"Adding 'sentencizer' to SpaCy English pipeline.\")\n",
        "    nlp_en.add_pipe(\"sentencizer\", first=True)\n",
        "\n",
        "aligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods=\"mai\")\n",
        "\n",
        "###############################################################################\n",
        "#                              HELPER FUNCTIONS\n",
        "###############################################################################\n",
        "\n",
        "def get_articles_in_category(category_title, lang):\n",
        "    \"\"\"\n",
        "    Fetches all article titles under a Wikipedia category.\n",
        "    \"\"\"\n",
        "    url = f'https://{lang}.wikipedia.org/w/api.php'\n",
        "    params = {\n",
        "        'action': 'query',\n",
        "        'list': 'categorymembers',\n",
        "        'cmtitle': f'Category:{category_title}',\n",
        "        'cmlimit': 'max',\n",
        "        'format': 'json',\n",
        "    }\n",
        "    articles = []\n",
        "    while True:\n",
        "        try:\n",
        "            response = session.get(url, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            for page in data['query']['categorymembers']:\n",
        "                if page['ns'] == 0:  # ns=0 indicates articles (not subcategories or files)\n",
        "                    articles.append(page['title'])\n",
        "            if 'continue' in data:\n",
        "                params['cmcontinue'] = data['continue']['cmcontinue']\n",
        "            else:\n",
        "                break\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logging.error(f\"Request failed for category '{category_title}' in '{lang}': {e}\")\n",
        "            break\n",
        "    return articles\n",
        "\n",
        "def get_langlinks(title, source_lang, target_lang):\n",
        "    \"\"\"\n",
        "    Checks if an article in source_lang has an equivalent article in target_lang.\n",
        "    Returns the title in target_lang if exists, else None.\n",
        "    \"\"\"\n",
        "    url = f'https://{source_lang}.wikipedia.org/w/api.php'\n",
        "    params = {\n",
        "        'action': 'query',\n",
        "        'titles': title,\n",
        "        'prop': 'langlinks',\n",
        "        'lllang': target_lang,\n",
        "        'format': 'json',\n",
        "    }\n",
        "    try:\n",
        "        response = session.get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        pages = data['query']['pages']\n",
        "        for page_id in pages:\n",
        "            if 'langlinks' in pages[page_id]:\n",
        "                return pages[page_id]['langlinks'][0]['*']\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Request failed for langlinks of page '{title}' from '{source_lang}' to '{target_lang}': {e}\")\n",
        "    return None\n",
        "\n",
        "def get_wikipedia_page(title, lang):\n",
        "    \"\"\"\n",
        "    Fetches the Wikipedia page content for a given title and language.\n",
        "    \"\"\"\n",
        "    url = f'https://{lang}.wikipedia.org/w/api.php'\n",
        "    params = {\n",
        "        'action': 'parse',\n",
        "        'page': title,\n",
        "        'prop': 'text',\n",
        "        'format': 'json',\n",
        "        'redirects': True,\n",
        "    }\n",
        "    try:\n",
        "        response = session.get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if 'error' in data:\n",
        "            logging.warning(f\"Error fetching page '{title}' in '{lang}': {data['error']['info']}\")\n",
        "            return None\n",
        "        html_content = data['parse']['text']['*']\n",
        "        return html_content\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Request failed for page '{title}' in '{lang}': {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_text(html_content):\n",
        "    \"\"\"\n",
        "    Extracts and cleans text from HTML content.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Remove certain tags which are not part of main text content\n",
        "    for element in soup(['table', 'sup', 'span', 'ol', 'ul', 'style', 'script']):\n",
        "        element.decompose()\n",
        "\n",
        "    text = soup.get_text(separator=' ')\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def split_into_sentences(text, lang):\n",
        "    \"\"\"\n",
        "    Splits text into sentences using appropriate tokenizer based on language.\n",
        "    \"\"\"\n",
        "    if lang == 'en':\n",
        "        doc = nlp_en(text)\n",
        "        sentences = [sent.text.strip() for sent in doc.sents]\n",
        "    else:\n",
        "        # For Indic languages, use the Indic NLP library\n",
        "        sentences = sentence_tokenize.sentence_split(text, lang)\n",
        "    return sentences\n",
        "\n",
        "def encode_sentences(sentences):\n",
        "    \"\"\"\n",
        "    Encodes sentences into vectors using a multilingual transformer model.\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    batch_size = 32\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(sentences), batch_size):\n",
        "            batch = sentences[i:i+batch_size]\n",
        "            inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "            outputs = model(**inputs)\n",
        "            batch_embeddings = outputs.pooler_output.cpu()\n",
        "            embeddings.append(batch_embeddings)\n",
        "    if embeddings:\n",
        "        embeddings = torch.cat(embeddings, dim=0)\n",
        "    else:\n",
        "        embeddings = torch.tensor([])\n",
        "    return embeddings\n",
        "\n",
        "def compute_similarity(embeddings_en, embeddings_target):\n",
        "    \"\"\"\n",
        "    Computes cosine similarity between English and target language sentence embeddings.\n",
        "    \"\"\"\n",
        "    if embeddings_en.size(0) == 0 or embeddings_target.size(0) == 0:\n",
        "        logging.warning(\"One of the embedding sets is empty. Cannot compute similarity.\")\n",
        "        return None\n",
        "\n",
        "    embeddings_en_np = embeddings_en.numpy()\n",
        "    embeddings_target_np = embeddings_target.numpy()\n",
        "\n",
        "    # Normalize each embedding vector\n",
        "    embeddings_en_norm = embeddings_en_np / np.linalg.norm(embeddings_en_np, axis=1, keepdims=True)\n",
        "    embeddings_target_norm = embeddings_target_np / np.linalg.norm(embeddings_target_np, axis=1, keepdims=True)\n",
        "\n",
        "    sim_matrix = cosine_similarity(embeddings_en_norm, embeddings_target_norm)\n",
        "    return sim_matrix\n",
        "\n",
        "def align_sentences(sentences_en, sentences_target, sim_matrix, threshold=0.8, alignment_threshold=0.8):\n",
        "    \"\"\"\n",
        "    Aligns sentences based on similarity scores with bidirectional best matching and filtering.\n",
        "    Uses word alignment from SimAlign to check alignment quality.\n",
        "\n",
        "    :param sentences_en: list of English sentences\n",
        "    :param sentences_target: list of target-language sentences\n",
        "    :param sim_matrix: computed similarity matrix\n",
        "    :param threshold: minimum similarity score for alignment\n",
        "    :param alignment_threshold: proportion of aligned words required to confirm alignment\n",
        "    :return: list of (en_sentence, tgt_sentence, alignment_score, sim_score)\n",
        "    \"\"\"\n",
        "    aligned_sentences = []\n",
        "\n",
        "    # For each English sentence, find the best target match\n",
        "    best_matches_en_to_tgt = sim_matrix.argmax(axis=1)\n",
        "    best_scores_en_to_tgt = sim_matrix.max(axis=1)\n",
        "\n",
        "    # For each target sentence, find the best English match\n",
        "    best_matches_tgt_to_en = sim_matrix.argmax(axis=0)\n",
        "    best_scores_tgt_to_en = sim_matrix.max(axis=0)\n",
        "\n",
        "    for en_idx, tgt_idx in enumerate(best_matches_en_to_tgt):\n",
        "        sim_score = best_scores_en_to_tgt[en_idx]\n",
        "        # Check if the similarity is above threshold\n",
        "        if sim_score < threshold:\n",
        "            continue\n",
        "\n",
        "        # Check if it's a mutual best match\n",
        "        if best_matches_tgt_to_en[tgt_idx] == en_idx and best_scores_tgt_to_en[tgt_idx] >= threshold:\n",
        "            en_sentence = sentences_en[en_idx]\n",
        "            tgt_sentence = sentences_target[tgt_idx]\n",
        "\n",
        "            # Length filtering\n",
        "            len_en = len(en_sentence.split())\n",
        "            len_tgt = len(tgt_sentence.split())\n",
        "            if min(len_en, len_tgt) == 0:\n",
        "                continue\n",
        "            length_ratio = max(len_en, len_tgt) / min(len_en, len_tgt)\n",
        "            if length_ratio > 1.3:\n",
        "                continue\n",
        "\n",
        "            # Word alignment using SimAlign\n",
        "            alignments = aligner.get_word_aligns(en_sentence, tgt_sentence)\n",
        "            num_aligned_words = len(alignments['inter'])\n",
        "            alignment_score = num_aligned_words / min(len_en, len_tgt)\n",
        "\n",
        "            if alignment_score >= alignment_threshold:\n",
        "                aligned_sentences.append((en_sentence, tgt_sentence, alignment_score, float(sim_score)))\n",
        "\n",
        "    return aligned_sentences\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#                              MAIN LOGIC\n",
        "###############################################################################\n",
        "\n",
        "def main(\n",
        "    langs: list[str] | None = None,\n",
        "    sim_thresholds: list[float] | None = None,\n",
        "    alpha_thresholds: list[float] | None = None,\n",
        "):\n",
        "    # reslove parameter overrides ------------------------------\n",
        "    if langs is None:\n",
        "        langs = INDIC_LANGUAGES\n",
        "    if sim_thresholds is None:\n",
        "        sim_thresholds = SIM_THRESHOLDS\n",
        "    if alpha_thresholds is None:\n",
        "        alpha_thresholds = ALIGN_THRESHOLDS\n",
        "\n",
        "    EN_LANG = 'en'\n",
        "\n",
        "    for lang_code in langs:\n",
        "        language_name = LANG_CODE_MAP.get(lang_code, lang_code)\n",
        "        logging.info(f\"==================== {language_name} ({lang_code}) \"\n",
        "                     \"====================\")\n",
        "\n",
        "        #######################################################################\n",
        "        # Containers for one single crawl pass\n",
        "        #######################################################################\n",
        "        all_pairs         = []      # every (sentence, sim, align, category)\n",
        "        article_category  = {}      # article_title -> first English category\n",
        "        #######################################################################\n",
        "\n",
        "        # ------------- 1. COLLECT ARTICLE TITLES (unchanged + mapping) ------\n",
        "        categories = language_categories.get(lang_code, [])\n",
        "        if not categories:\n",
        "            logging.warning(f\"No categories specified for {language_name}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        article_titles = set()\n",
        "        for cat in categories:\n",
        "            titles_en = get_articles_in_category(cat, EN_LANG)\n",
        "            for t in titles_en:\n",
        "                article_category.setdefault(t, cat)   # store only first cat\n",
        "            article_titles.update(titles_en)\n",
        "\n",
        "        total_articles_fetched = len(article_titles)\n",
        "        logging.info(f\"Found {total_articles_fetched} unique English articles.\")\n",
        "\n",
        "        # ------ 2. PROCESS EACH BILINGUAL ARTICLE (download & align once) ----\n",
        "        titles_with_langlinks = 0\n",
        "        for title in tqdm(article_titles,\n",
        "                          desc=f\"Langlinks > {language_name}\",\n",
        "                          leave=False):\n",
        "            tgt_title = get_langlinks(title, EN_LANG, lang_code)\n",
        "            if tgt_title is None:\n",
        "                continue\n",
        "            titles_with_langlinks += 1\n",
        "\n",
        "            html_en  = get_wikipedia_page(title,     EN_LANG)\n",
        "            html_tgt = get_wikipedia_page(tgt_title, lang_code)\n",
        "            if not html_en or not html_tgt:\n",
        "                continue\n",
        "\n",
        "            text_en  = extract_text(html_en)\n",
        "            text_tgt = extract_text(html_tgt)\n",
        "            if not text_en or not text_tgt:\n",
        "                continue\n",
        "\n",
        "            # ---- sentence split & length filter ----------------------------\n",
        "            def filter_sents(sents, lo=5, hi=200):\n",
        "                return [s for s in sents if lo <= len(s.split()) <= hi]\n",
        "\n",
        "            sents_en  = filter_sents(split_into_sentences(text_en,  EN_LANG))\n",
        "            sents_tgt = filter_sents(split_into_sentences(text_tgt, lang_code))\n",
        "            if not sents_en or not sents_tgt:\n",
        "                continue\n",
        "\n",
        "            # ---- embed once -------------------------------------------------\n",
        "            emb_en  = encode_sentences(sents_en)\n",
        "            emb_tgt = encode_sentences(sents_tgt)\n",
        "            sim_mtx = compute_similarity(emb_en, emb_tgt)\n",
        "            if sim_mtx is None:\n",
        "                continue\n",
        "\n",
        "            # ---- capture all potential pairs (τ=0, α=0) ----------------\n",
        "            aligned_raw = align_sentences(\n",
        "                sents_en, sents_tgt, sim_mtx,\n",
        "                threshold=0.4, alignment_threshold=0.0\n",
        "            )\n",
        "\n",
        "            cat = article_category[title]\n",
        "            for (sen_en, sen_tgt, align_sc, sim_sc) in aligned_raw:\n",
        "                all_pairs.append({\n",
        "                    \"en\": sen_en,\n",
        "                    \"tgt\": sen_tgt,\n",
        "                    \"sim\": sim_sc,\n",
        "                    \"align\": align_sc,\n",
        "                    \"category\": cat\n",
        "                })\n",
        "        logging.info(f\"{titles_with_langlinks}/{total_articles_fetched} \"\n",
        "                     f\"articles had {language_name} counterparts.\")\n",
        "        logging.info(f\"Total raw sentence pairs harvested: {len(all_pairs)}\")\n",
        "\n",
        "        # ---------- 3. MULTI‑THRESHOLD OUTPUT LOOP --------------------------\n",
        "        for tau in sim_thresholds:\n",
        "            for alpha in alpha_thresholds:\n",
        "                # Directory structure:  parallel_datasets/<τ>/<α>/…\n",
        "                ds_dir = os.path.join(\"parallel_datasets\", str(tau), str(alpha))\n",
        "                st_dir = os.path.join(\"stats\",             str(tau), str(alpha))\n",
        "                os.makedirs(ds_dir, exist_ok=True)\n",
        "                os.makedirs(st_dir, exist_ok=True)\n",
        "\n",
        "                # Filter pairs that satisfy the current grid point\n",
        "                selected = [\n",
        "                    p for p in all_pairs\n",
        "                    if p[\"sim\"] >= tau and p[\"align\"] >= alpha\n",
        "                ]\n",
        "\n",
        "                # ---------- PARALLEL DATASET JSONL --------------------------\n",
        "                df = pd.DataFrame(\n",
        "                    [(p[\"en\"], p[\"tgt\"]) for p in selected],\n",
        "                    columns=[\"English\", language_name]\n",
        "                )\n",
        "                ds_path = os.path.join(\n",
        "                    ds_dir, f\"parallel_dataset_{language_name.lower()}.json\"\n",
        "                )\n",
        "                df.to_json(ds_path, orient=\"records\",\n",
        "                           lines=True, force_ascii=False)\n",
        "\n",
        "                # ---------- STATS (aggregate + per‑category) ---------------\n",
        "                agg = {\n",
        "                    \"language\": language_name,\n",
        "                    \"tau\": tau,\n",
        "                    \"alpha\": alpha,\n",
        "                    \"num_pairs\": len(selected),\n",
        "                    \"mean_cosine\": (float(np.mean([p[\"sim\"]\n",
        "                                      for p in selected]))\n",
        "                                    if selected else 0.0),\n",
        "                    \"mean_alignment\": (float(np.mean([p[\"align\"]\n",
        "                                         for p in selected]))\n",
        "                                       if selected else 0.0)\n",
        "                }\n",
        "                by_cat = {}\n",
        "                for cat in categories:\n",
        "                    cp = [p for p in selected if p[\"category\"] == cat]\n",
        "                    if cp:\n",
        "                        by_cat[cat] = {\n",
        "                            \"num_pairs\": len(cp),\n",
        "                            \"mean_cosine\": float(np.mean([p[\"sim\"] for p in cp])),\n",
        "                            \"mean_alignment\": float(np.mean([p[\"align\"]\n",
        "                                                             for p in cp]))\n",
        "                        }\n",
        "\n",
        "                st_path = os.path.join(\n",
        "                    st_dir, f\"stats_{language_name.lower()}.json\"\n",
        "                )\n",
        "                with open(st_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump({\"aggregate\": agg, \"by_category\": by_cat},\n",
        "                              f, ensure_ascii=False, indent=4)\n",
        "\n",
        "                logging.info(f\"[{language_name}] τ={tau} α={alpha} \"\n",
        "                             f\"→ {len(selected)} pairs \"\n",
        "                             f\"(saved: {ds_path})\")\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "if __name__ == \"__main__\":\n",
        "    #default execution\n",
        "    main()\n",
        "\n",
        "    # run just Hindi with one grid point\n",
        "    # main(\n",
        "    #     langs=['hi'],\n",
        "    #     sim_thresholds=[0.7],    # τ\n",
        "    #     alpha_thresholds=[0.6]   # α\n",
        "    # )\n",
        "\n",
        "    # # run Bengali + Gujarati, full default grids\n",
        "    # main(langs=['bn', 'gu'])\n",
        "\n",
        "    # # run everything but with a finer τ grid\n",
        "    # main(sim_thresholds=[0.55, 0.6, 0.7, 0.8])\n",
        "\n",
        "\n",
        "    # # Should run one small grid without errors\n",
        "    # main(langs=['as'], sim_thresholds=[0.7], alpha_thresholds=[0.6])\n"
      ],
      "metadata": {
        "id": "8RPpeakc2QJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1.  Imports & setup\n",
        "# ------------------------------------------------------------\n",
        "import json, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "STATS_ROOT = \"stats\"\n",
        "TAU_GRID   = [0.6, 0.7, 0.8]\n",
        "ALPHA_GRID = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "LANGS      = [\"as\",\"bn\",\"gu\",\"hi\",\"kn\",\"ml\",\"mr\",\"or\",\"pa\",\"ta\",\"te\"]\n",
        "LANG_FULL = {\n",
        "    'as':'Assamese','bn':'Bengali','gu':'Gujarati','hi':'Hindi',\n",
        "    'kn':'Kannada','ml':'Malayalam','mr':'Marathi','or':'Oriya',\n",
        "    'pa':'Punjabi','ta':'Tamil','te':'Telugu'\n",
        "}\n",
        "\n",
        "def load_stats(lang, tau, alpha):\n",
        "    path = os.path.join(STATS_ROOT, str(tau), str(alpha),\n",
        "                        f\"stats_{LANG_FULL[lang].lower()}.json\")\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2.  Sweet‑spot heat‑map (robust to missing files)\n",
        "# ------------------------------------------------------------\n",
        "fig, axes = plt.subplots(4, 3, figsize=(15,20))\n",
        "axes = axes.flatten()\n",
        "cmap = plt.get_cmap('plasma')\n",
        "\n",
        "for idx, lang in enumerate(LANGS):\n",
        "    grid_cos = np.zeros((len(ALPHA_GRID), len(TAU_GRID)))\n",
        "    grid_cnt = np.zeros_like(grid_cos, dtype=int)\n",
        "\n",
        "    for i, alpha in enumerate(ALPHA_GRID):\n",
        "        for j, tau in enumerate(TAU_GRID):\n",
        "            stats = load_stats(lang, tau, alpha)\n",
        "            if stats:\n",
        "                grid_cos[i,j] = stats[\"aggregate\"][\"mean_cosine\"]\n",
        "                grid_cnt[i,j] = stats[\"aggregate\"][\"num_pairs\"]\n",
        "            else:\n",
        "                grid_cos[i,j] = np.nan\n",
        "                grid_cnt[i,j] = 0\n",
        "\n",
        "    ax = axes[idx]\n",
        "    im = ax.imshow(grid_cos, aspect=\"auto\", cmap=cmap, vmin=0.5, vmax=1.0)\n",
        "    ax.set_xticks(range(len(TAU_GRID))); ax.set_xticklabels(TAU_GRID, rotation=45)\n",
        "    ax.set_yticks(range(len(ALPHA_GRID))); ax.set_yticklabels(ALPHA_GRID)\n",
        "    ax.set_title(LANG_FULL[lang], pad=10)\n",
        "\n",
        "    for i in range(len(ALPHA_GRID)):\n",
        "        for j in range(len(TAU_GRID)):\n",
        "            val = grid_cos[i,j]\n",
        "            count = grid_cnt[i,j]\n",
        "            if np.isnan(val):\n",
        "                txt = \"–\\n0\"\n",
        "                color = \"grey\"\n",
        "            else:\n",
        "                txt = f\"{val:.2f}\\n{count}\"\n",
        "                color = 'white' if val<0.75 else 'black'\n",
        "            ax.text(j, i, txt, ha=\"center\", va=\"center\", fontsize=8, color=color)\n",
        "\n",
        "fig.subplots_adjust(right=0.85, top=0.92)\n",
        "cbar_ax = fig.add_axes([0.88,0.15,0.02,0.7])\n",
        "fig.colorbar(im, cax=cbar_ax, label='Mean Cosine')\n",
        "fig.suptitle(\"Sweet‑spot Heatmap\", fontsize=16)\n",
        "plt.tight_layout(rect=[0,0,0.85,0.96])\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3.  Category bar‑chart for τ=0.7, α=0.6\n",
        "# ------------------------------------------------------------\n",
        "SAMPLE_TAU, SAMPLE_ALPHA = 0.7, 0.6\n",
        "for lang in LANGS:\n",
        "    stats = load_stats(lang, SAMPLE_TAU, SAMPLE_ALPHA)\n",
        "    if not stats or not stats.get(\"by_category\"):\n",
        "        continue\n",
        "    df_cat = pd.DataFrame.from_dict(stats[\"by_category\"], orient=\"index\")\n",
        "    df_cat[\"num_pairs\"] = df_cat.get(\"num_pairs\", 0)\n",
        "    df_cat = df_cat.sort_values(\"num_pairs\", ascending=False).head(5)\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.barh(df_cat.index[::-1], df_cat[\"num_pairs\"][::-1], edgecolor='k')\n",
        "    plt.title(f\"Top‑5 Categories: {LANG_FULL[lang]} (τ={SAMPLE_TAU}, α={SAMPLE_ALPHA})\", pad=8)\n",
        "    plt.xlabel(\"Sentence Pairs\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4.  Executive table for τ=0.7, α=0.6\n",
        "# ------------------------------------------------------------\n",
        "rows = []\n",
        "for lang in LANGS:\n",
        "    stats = load_stats(lang, SAMPLE_TAU, SAMPLE_ALPHA)\n",
        "    if not stats:\n",
        "        rows.append({\"Language\": LANG_FULL[lang], \"Pairs\":0, \"Mean Cos\":0, \"Mean Align\":0})\n",
        "    else:\n",
        "        agg = stats[\"aggregate\"]\n",
        "        rows.append({\n",
        "            \"Language\": LANG_FULL[lang],\n",
        "            \"Pairs\":     agg[\"num_pairs\"],\n",
        "            \"Mean Cos\":  round(agg[\"mean_cosine\"],3),\n",
        "            \"Mean Align\":round(agg[\"mean_alignment\"],3)\n",
        "        })\n",
        "exec_df = pd.DataFrame(rows).sort_values(\"Pairs\", ascending=False).reset_index(drop=True)\n",
        "display(exec_df)        # Jupyter/Colab will render as a neat table\n",
        "\n",
        "# also save as CSV if you like\n",
        "exec_df.to_csv(\"executive_table_0.6_0.7.csv\", index=False)"
      ],
      "metadata": {
        "id": "dmiqTJwy7CQX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}