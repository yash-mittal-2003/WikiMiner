{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers datasets evaluate seqeval --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5enZo9J0KfbW",
        "outputId": "8369a88d-f700-4838-ac79-d55869418982"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# Indic‑NER fine‑tuning on Naamapadam (+ optional custom JSON) – v2.1\n",
        "# – minimal edits to your original “working” code\n",
        "###############################################################################\n",
        "import argparse, random, json, os, itertools, logging, collections\n",
        "import numpy as np, torch\n",
        "from datasets import load_dataset, concatenate_datasets, DatasetDict, Sequence, Value\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    DataCollatorForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 0.  LOGGING\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
        "    level=logging.INFO,\n",
        "    datefmt=\"%H:%M:%S\",\n",
        ")\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 1.  ARGUMENTS & SEEDING\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model_name\", type=str, default=\"ai4bharat/indic-bert\")\n",
        "parser.add_argument(\"--languages\", nargs=\"+\", default=[\"as\"])\n",
        "parser.add_argument(\"--custom_data_path\", type=str, default=\"/content/drive/MyDrive/naamapadam_proj/0.6/0.4/naamapadam_assamese.json\",\n",
        "                    help=\"Path to your extra JSON file (optional)\")\n",
        "parser.add_argument(\"--output_dir\", type=str,\n",
        "                    default=\"./indicner-finetuned-naamapadam\")\n",
        "parser.add_argument(\"--num_train_epochs\", type=int, default=10)\n",
        "parser.add_argument(\"--per_device_train_batch_size\", type=int, default=32)\n",
        "parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=32)\n",
        "parser.add_argument(\"--learning_rate\", type=float, default=1e-5)\n",
        "parser.add_argument(\"--weight_decay\",  type=float, default=0.01)\n",
        "parser.add_argument(\"--warmup_steps\",  type=int, default=500)\n",
        "parser.add_argument(\"--seed\", type=int, default=42)\n",
        "args, _ = parser.parse_known_args()\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "    log.info(\"CUDA device: %s\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    log.info(\"CUDA not available – falling back to CPU\")\n",
        "\n",
        "lang = args.languages[0]          # single‑language run                                  #\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 2.  CANONICAL LABEL SET  (Naamapadam has exactly 7)\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "label_list = [\"B-LOC\",\"B-ORG\",\"B-PER\",\"I-LOC\",\"I-ORG\",\"I-PER\",\"O\"]\n",
        "id2label   = {i:l for i,l in enumerate(label_list)}  # type: ignore\n",
        "label2id   = {l:i for i,l in id2label.items()}\n",
        "num_labels = len(label_list)\n",
        "log.info(\"Canonical labels: %s\", id2label)\n",
        "\n",
        "# Helper – map *any* tag outside this list to “O”\n",
        "def normalise_tag(tag: str) -> str:\n",
        "    tag = str(tag)\n",
        "    if tag in (\"B-PERSON\", \"I-PERSON\"):         # your PERSON alias\n",
        "        tag = tag.replace(\"PERSON\", \"PER\")\n",
        "    return tag if tag in label2id else \"O\"\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 3.  LOAD NAAMAPADAM (train / test)  –  they’re already ints\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "train_ref = load_dataset(\"ai4bharat/naamapadam\", lang, split=\"train\")\n",
        "test_ref  = load_dataset(\"ai4bharat/naamapadam\", lang, split=\"test\")\n",
        "\n",
        "# NEW  ➜  make their `ner_tags` column a plain int sequence\n",
        "int_seq = Sequence(Value(\"int64\"))\n",
        "train_ref = train_ref.cast_column(\"ner_tags\", int_seq)\n",
        "test_ref  = test_ref.cast_column(\"ner_tags\", int_seq)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 4.  OPTIONAL: LOAD + CLEAN YOUR CUSTOM JSON\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "if args.custom_data_path:\n",
        "    log.info(\"Loading custom JSON: %s\", args.custom_data_path)\n",
        "    custom = load_dataset(\"json\", data_files=args.custom_data_path,\n",
        "                          split=\"train\")\n",
        "\n",
        "    # (a) ensure tokens / ner_tags length match\n",
        "    broken = [i for i, ex in enumerate(custom)\n",
        "              if len(ex[\"tokens\"]) != len(ex[\"ner_tags\"])]\n",
        "    if broken:\n",
        "        log.warning(\"⚠️  %d rows have mismatching lengths – they’ll be dropped\",\n",
        "                    len(broken))\n",
        "        custom = custom.select([i for i in range(len(custom)) if i not in broken])\n",
        "\n",
        "    # (b) normalise & map to ints\n",
        "    def _clean(batch):\n",
        "        out = []\n",
        "        for tags in batch[\"ner_tags\"]:\n",
        "            out.append([label2id[normalise_tag(t)] for t in tags])\n",
        "        batch[\"ner_tags\"] = out\n",
        "        return batch\n",
        "\n",
        "    custom = custom.map(_clean, batched=True)\n",
        "    log.info(\"Custom set after cleaning: %d sentences\", len(custom))\n",
        "    train_all = concatenate_datasets([train_ref, custom])\n",
        "else:\n",
        "    train_all = train_ref\n",
        "\n",
        "# quick label‑distribution print‑out\n",
        "def label_hist(ds, name):\n",
        "    flat = list(itertools.chain.from_iterable(ds[\"ner_tags\"]))\n",
        "    c = collections.Counter(flat)\n",
        "    log.info(\"%s label distribution: %s\",\n",
        "             name, {id2label[k]: v for k,v in c.items()})\n",
        "label_hist(train_all, \"TRAIN\")\n",
        "label_hist(test_ref,  \"TEST \")\n",
        "\n",
        "# 5. TRAIN / DEV SPLIT ---------------------------------------------------------\n",
        "split = train_all.train_test_split(train_size=0.75, seed=args.seed)\n",
        "train_ds, dev_ds = split[\"train\"], split[\"test\"]\n",
        "log.info(\"Train %d  |  Dev %d  |  Test %d\", len(train_ds), len(dev_ds), len(test_ref))\n",
        "\n",
        "# 6. TOKENISATION + LABEL ALIGNMENT -------------------------------------------\n",
        "tok = AutoTokenizer.from_pretrained(args.model_name, use_fast=True)\n",
        "def align(batch):\n",
        "    enc = tok(batch[\"tokens\"], is_split_into_words=True,\n",
        "              truncation=True, max_length=512)\n",
        "    new_labels = []\n",
        "    for i, seq in enumerate(batch[\"ner_tags\"]):\n",
        "        word_ids = enc.word_ids(batch_index=i)\n",
        "        prev = None\n",
        "        aligned = []\n",
        "        for w in word_ids:\n",
        "            if w is None:\n",
        "                aligned.append(-100)\n",
        "            elif w != prev:\n",
        "                aligned.append(seq[w])\n",
        "            else:\n",
        "                aligned.append(-100)\n",
        "            prev = w\n",
        "        new_labels.append(aligned)\n",
        "    enc[\"labels\"] = new_labels\n",
        "    return enc\n",
        "\n",
        "train_ds = train_ds.map(align, batched=True, remove_columns=[\"ner_tags\"])\n",
        "dev_ds   = dev_ds  .map(align, batched=True, remove_columns=[\"ner_tags\"])\n",
        "test_ref = test_ref.map(align, batched=True, remove_columns=[\"ner_tags\"])\n",
        "\n",
        "# 7. MODEL, METRICS, TRAINER ---------------------------------------------------\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "def compute_metrics(p):\n",
        "    preds, labs = p\n",
        "    preds = np.argmax(preds, axis=2)\n",
        "    true_preds, true_labs = [], []\n",
        "    for pr, lb in zip(preds, labs):\n",
        "        pr_l, lb_l = [], []\n",
        "        for p_i, l_i in zip(pr, lb):\n",
        "            if l_i != -100:\n",
        "                pr_l.append(id2label[p_i])\n",
        "                lb_l.append(id2label[l_i])\n",
        "        true_preds.append(pr_l)\n",
        "        true_labs.append(lb_l)\n",
        "    res = metric.compute(predictions=true_preds, references=true_labs,\n",
        "                         zero_division=0)\n",
        "    return {k.replace(\"overall_\", \"\"): v for k,v in res.items()}\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    args.model_name, num_labels=num_labels,\n",
        "    id2label=id2label, label2id=label2id)\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    output_dir     = args.output_dir,\n",
        "    eval_strategy=\"epoch\",           # ← one eval *after* each epoch\n",
        "    save_strategy  =\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    num_train_epochs=args.num_train_epochs,\n",
        "    per_device_train_batch_size=args.per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
        "    learning_rate = args.learning_rate,\n",
        "    weight_decay  = args.weight_decay,\n",
        "    warmup_steps  = args.warmup_steps,\n",
        "    seed = args.seed,\n",
        "    fp16 = torch.cuda.is_available(),\n",
        "    report_to=\"none\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=train_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset =dev_ds,\n",
        "    tokenizer=tok,\n",
        "    data_collator=DataCollatorForTokenClassification(tok),\n",
        "    compute_metrics=compute_metrics)\n",
        "\n",
        "# 8. TRAIN & FINAL EVALUATION --------------------------------------------------\n",
        "log.info(\"⏳  Starting fine‑tuning …\")\n",
        "trainer.train()\n",
        "log.info(\"✅  Finished training.  Best dev‑set F1: %.4f\",\n",
        "         trainer.state.best_metric or -1)\n",
        "\n",
        "log.info(\"🏁  Test‑set metrics:\")\n",
        "print(trainer.evaluate(eval_dataset=test_ref))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915,
          "referenced_widgets": [
            "12c5d32d439b42b8af01104042eadf98",
            "153b56b5cb2d41cc95f35ef852643136",
            "dd6fe730c4aa47c5965290330b739d02",
            "b2bf17e1ed944262a56b5e4a24eee4dd",
            "4b0ac7fa63a3458c8163e9a603ccc61f",
            "47282e20622945b2bd1c5c0af85a15f3",
            "181c7e5412514a29b15dc408f8d297e3",
            "45d401407215434c8acc072a628e4092",
            "e57566cb114649e2907d20432694ae81",
            "815af6151c60480a9103b5a0444f3e04",
            "4861aa5fae734f84bd745655daae6aa1"
          ]
        },
        "id": "NCWymXFdRS4g",
        "outputId": "ba1f61ca-23a5-429c-e288-858ccbe45a9c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12c5d32d439b42b8af01104042eadf98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-6-e9ff764aa434>:192: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2750' max='2750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2750/2750 07:00, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Loc</th>\n",
              "      <th>Org</th>\n",
              "      <th>Per</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.757820</td>\n",
              "      <td>{'precision': 0.756083517682354, 'recall': 0.9820128311095367, 'f1': 0.854364277642708, 'number': 25407}</td>\n",
              "      <td>{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 773}</td>\n",
              "      <td>{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1087}</td>\n",
              "      <td>0.756084</td>\n",
              "      <td>0.915025</td>\n",
              "      <td>0.827996</td>\n",
              "      <td>0.761127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.979900</td>\n",
              "      <td>0.735514</td>\n",
              "      <td>{'precision': 0.7727244715597461, 'recall': 0.9870508127681348, 'f1': 0.8668360380912878, 'number': 25407}</td>\n",
              "      <td>{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 773}</td>\n",
              "      <td>{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1087}</td>\n",
              "      <td>0.772724</td>\n",
              "      <td>0.919720</td>\n",
              "      <td>0.839839</td>\n",
              "      <td>0.783710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.979900</td>\n",
              "      <td>0.662636</td>\n",
              "      <td>{'precision': 0.9054802045041619, 'recall': 0.8434683354980911, 'f1': 0.873374903207401, 'number': 25407}</td>\n",
              "      <td>{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 773}</td>\n",
              "      <td>{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1087}</td>\n",
              "      <td>0.905480</td>\n",
              "      <td>0.785932</td>\n",
              "      <td>0.841481</td>\n",
              "      <td>0.793987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.578200</td>\n",
              "      <td>0.576259</td>\n",
              "      <td>{'precision': 0.8744743637034305, 'recall': 0.933089306096745, 'f1': 0.9028314640973399, 'number': 25407}</td>\n",
              "      <td>{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 773}</td>\n",
              "      <td>{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1087}</td>\n",
              "      <td>0.874474</td>\n",
              "      <td>0.869439</td>\n",
              "      <td>0.871950</td>\n",
              "      <td>0.836383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.578200</td>\n",
              "      <td>0.569003</td>\n",
              "      <td>{'precision': 0.8732072410869004, 'recall': 0.9321840437674657, 'f1': 0.901732343422806, 'number': 25407}</td>\n",
              "      <td>{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 773}</td>\n",
              "      <td>{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1087}</td>\n",
              "      <td>0.873207</td>\n",
              "      <td>0.868596</td>\n",
              "      <td>0.870895</td>\n",
              "      <td>0.835327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.473600</td>\n",
              "      <td>0.578521</td>\n",
              "      <td>{'precision': 0.8633090948167783, 'recall': 0.948596843389617, 'f1': 0.9039456904958367, 'number': 25407}</td>\n",
              "      <td>{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 773}</td>\n",
              "      <td>{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1087}</td>\n",
              "      <td>0.863216</td>\n",
              "      <td>0.883889</td>\n",
              "      <td>0.873430</td>\n",
              "      <td>0.841636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.473600</td>\n",
              "      <td>0.593578</td>\n",
              "      <td>{'precision': 0.8692061413083024, 'recall': 0.9403314047309796, 'f1': 0.9033709564592669, 'number': 25407}</td>\n",
              "      <td>{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 773}</td>\n",
              "      <td>{'precision': 0.5806451612903226, 'recall': 0.01655933762649494, 'f1': 0.032200357781753126, 'number': 1087}</td>\n",
              "      <td>0.868629</td>\n",
              "      <td>0.876847</td>\n",
              "      <td>0.872719</td>\n",
              "      <td>0.839381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.384800</td>\n",
              "      <td>0.600378</td>\n",
              "      <td>{'precision': 0.8788919584857762, 'recall': 0.9265950328649585, 'f1': 0.9021133102140134, 'number': 25407}</td>\n",
              "      <td>{'precision': 0.06666666666666667, 'recall': 0.00129366106080207, 'f1': 0.0025380710659898475, 'number': 773}</td>\n",
              "      <td>{'precision': 0.6101694915254238, 'recall': 0.06623735050597976, 'f1': 0.11950207468879667, 'number': 1087}</td>\n",
              "      <td>0.877261</td>\n",
              "      <td>0.866065</td>\n",
              "      <td>0.871627</td>\n",
              "      <td>0.837354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.384800</td>\n",
              "      <td>0.586606</td>\n",
              "      <td>{'precision': 0.875160403299725, 'recall': 0.9395048608651159, 'f1': 0.9061918681902738, 'number': 25407}</td>\n",
              "      <td>{'precision': 0.05263157894736842, 'recall': 0.00129366106080207, 'f1': 0.0025252525252525255, 'number': 773}</td>\n",
              "      <td>{'precision': 0.5511111111111111, 'recall': 0.1140754369825207, 'f1': 0.1890243902439024, 'number': 1087}</td>\n",
              "      <td>0.871943</td>\n",
              "      <td>0.880001</td>\n",
              "      <td>0.875954</td>\n",
              "      <td>0.844491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.338600</td>\n",
              "      <td>0.589701</td>\n",
              "      <td>{'precision': 0.8769842731391109, 'recall': 0.9371826661943559, 'f1': 0.9060847064195745, 'number': 25407}</td>\n",
              "      <td>{'precision': 0.08, 'recall': 0.00258732212160414, 'f1': 0.005012531328320802, 'number': 773}</td>\n",
              "      <td>{'precision': 0.5714285714285714, 'recall': 0.12879484820607176, 'f1': 0.2102102102102102, 'number': 1087}</td>\n",
              "      <td>0.873528</td>\n",
              "      <td>0.878461</td>\n",
              "      <td>0.875987</td>\n",
              "      <td>0.843749</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.3034144341945648, 'eval_LOC': {'precision': 0.9165085388994307, 'recall': 0.971830985915493, 'f1': 0.9433593750000001, 'number': 497}, 'eval_ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 9}, 'eval_PER': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 11}, 'eval_precision': 0.9165085388994307, 'eval_recall': 0.9342359767891683, 'eval_f1': 0.925287356321839, 'eval_accuracy': 0.9123134328358209, 'eval_runtime': 0.1058, 'eval_samples_per_second': 482.035, 'eval_steps_per_second': 18.903, 'epoch': 10.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# Indic‑NER fine‑tuning on Naamapadam (+ optional custom JSON) – v2.1\n",
        "# – minimal edits to your original “working” code\n",
        "###############################################################################\n",
        "import argparse, random, json, os, itertools, logging, collections\n",
        "import numpy as np, torch\n",
        "from datasets import load_dataset, concatenate_datasets, DatasetDict, Sequence, Value\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    DataCollatorForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 0.  LOGGING\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
        "    level=logging.INFO,\n",
        "    datefmt=\"%H:%M:%S\",\n",
        ")\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 1.  ARGUMENTS & SEEDING\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model_name\", type=str, default=\"ai4bharat/indic-bert\")\n",
        "parser.add_argument(\"--languages\", nargs=\"+\", default=[\"as\"])\n",
        "parser.add_argument(\"--custom_data_path\", type=str, default=None,\n",
        "                    help=\"Path to your extra JSON file (optional)\")\n",
        "parser.add_argument(\"--output_dir\", type=str,\n",
        "                    default=\"./indicner-finetuned-naamapadam\")\n",
        "parser.add_argument(\"--num_train_epochs\", type=int, default=5)\n",
        "parser.add_argument(\"--per_device_train_batch_size\", type=int, default=32)\n",
        "parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=32)\n",
        "parser.add_argument(\"--learning_rate\", type=float, default=3e-5)\n",
        "parser.add_argument(\"--weight_decay\",  type=float, default=0.01)\n",
        "parser.add_argument(\"--warmup_steps\",  type=int, default=500)\n",
        "parser.add_argument(\"--seed\", type=int, default=42)\n",
        "args, _ = parser.parse_known_args()\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "    log.info(\"CUDA device: %s\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    log.info(\"CUDA not available – falling back to CPU\")\n",
        "\n",
        "lang = args.languages[0]          # single‑language run\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 2.  CANONICAL LABEL SET  (Naamapadam has exactly 7)\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "label_list = [\"B-LOC\",\"B-ORG\",\"B-PER\",\"I-LOC\",\"I-ORG\",\"I-PER\",\"O\"]\n",
        "id2label   = {i:l for i,l in enumerate(label_list)}  # type: ignore\n",
        "label2id   = {l:i for i,l in id2label.items()}\n",
        "num_labels = len(label_list)\n",
        "log.info(\"Canonical labels: %s\", id2label)\n",
        "\n",
        "# Helper – map *any* tag outside this list to “O”\n",
        "def normalise_tag(tag: str) -> str:\n",
        "    tag = str(tag)\n",
        "    if tag in (\"B-PERSON\", \"I-PERSON\"):         # your PERSON alias\n",
        "        tag = tag.replace(\"PERSON\", \"PER\")\n",
        "    return tag if tag in label2id else \"O\"\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 3.  LOAD NAAMAPADAM (train / test)  –  they’re already ints\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "train_ref = load_dataset(\"ai4bharat/naamapadam\", lang, split=\"train\")\n",
        "test_ref  = load_dataset(\"ai4bharat/naamapadam\", lang, split=\"test\")\n",
        "\n",
        "# NEW  ➜  make their `ner_tags` column a plain int sequence\n",
        "int_seq = Sequence(Value(\"int64\"))\n",
        "train_ref = train_ref.cast_column(\"ner_tags\", int_seq)\n",
        "test_ref  = test_ref.cast_column(\"ner_tags\", int_seq)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "# 4.  OPTIONAL: LOAD + CLEAN YOUR CUSTOM JSON\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "if args.custom_data_path:\n",
        "    log.info(\"Loading custom JSON: %s\", args.custom_data_path)\n",
        "    custom = load_dataset(\"json\", data_files=args.custom_data_path,\n",
        "                          split=\"train\")\n",
        "\n",
        "    # (a) ensure tokens / ner_tags length match\n",
        "    broken = [i for i, ex in enumerate(custom)\n",
        "              if len(ex[\"tokens\"]) != len(ex[\"ner_tags\"])]\n",
        "    if broken:\n",
        "        log.warning(\"⚠️  %d rows have mismatching lengths – they’ll be dropped\",\n",
        "                    len(broken))\n",
        "        custom = custom.select([i for i in range(len(custom)) if i not in broken])\n",
        "\n",
        "    # (b) normalise & map to ints\n",
        "    def _clean(batch):\n",
        "        out = []\n",
        "        for tags in batch[\"ner_tags\"]:\n",
        "            out.append([label2id[normalise_tag(t)] for t in tags])\n",
        "        batch[\"ner_tags\"] = out\n",
        "        return batch\n",
        "\n",
        "    custom = custom.map(_clean, batched=True)\n",
        "    log.info(\"Custom set after cleaning: %d sentences\", len(custom))\n",
        "    train_all = concatenate_datasets([train_ref, custom])\n",
        "else:\n",
        "    train_all = train_ref\n",
        "\n",
        "# quick label‑distribution print‑out\n",
        "def label_hist(ds, name):\n",
        "    flat = list(itertools.chain.from_iterable(ds[\"ner_tags\"]))\n",
        "    c = collections.Counter(flat)\n",
        "    log.info(\"%s label distribution: %s\",\n",
        "             name, {id2label[k]: v for k,v in c.items()})\n",
        "label_hist(train_all, \"TRAIN\")\n",
        "label_hist(test_ref,  \"TEST \")\n",
        "\n",
        "# 5. TRAIN / DEV SPLIT ---------------------------------------------------------\n",
        "split = train_all.train_test_split(train_size=0.75, seed=args.seed)\n",
        "train_ds, dev_ds = split[\"train\"], split[\"test\"]\n",
        "log.info(\"Train %d  |  Dev %d  |  Test %d\", len(train_ds), len(dev_ds), len(test_ref))\n",
        "\n",
        "# 6. TOKENISATION + LABEL ALIGNMENT -------------------------------------------\n",
        "tok = AutoTokenizer.from_pretrained(args.model_name, use_fast=True)\n",
        "def align(batch):\n",
        "    enc = tok(batch[\"tokens\"], is_split_into_words=True,\n",
        "              truncation=True, max_length=512)\n",
        "    new_labels = []\n",
        "    for i, seq in enumerate(batch[\"ner_tags\"]):\n",
        "        word_ids = enc.word_ids(batch_index=i)\n",
        "        prev = None\n",
        "        aligned = []\n",
        "        for w in word_ids:\n",
        "            if w is None:\n",
        "                aligned.append(-100)\n",
        "            elif w != prev:\n",
        "                aligned.append(seq[w])\n",
        "            else:\n",
        "                aligned.append(-100)\n",
        "            prev = w\n",
        "        new_labels.append(aligned)\n",
        "    enc[\"labels\"] = new_labels\n",
        "    return enc\n",
        "\n",
        "train_ds = train_ds.map(align, batched=True, remove_columns=[\"ner_tags\"])\n",
        "dev_ds   = dev_ds  .map(align, batched=True, remove_columns=[\"ner_tags\"])\n",
        "test_ref = test_ref.map(align, batched=True, remove_columns=[\"ner_tags\"])\n",
        "\n",
        "# 7. MODEL, METRICS, TRAINER ---------------------------------------------------\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "def compute_metrics(p):\n",
        "    preds, labs = p\n",
        "    preds = np.argmax(preds, axis=2)\n",
        "    true_preds, true_labs = [], []\n",
        "    for pr, lb in zip(preds, labs):\n",
        "        pr_l, lb_l = [], []\n",
        "        for p_i, l_i in zip(pr, lb):\n",
        "            if l_i != -100:\n",
        "                pr_l.append(id2label[p_i])\n",
        "                lb_l.append(id2label[l_i])\n",
        "        true_preds.append(pr_l)\n",
        "        true_labs.append(lb_l)\n",
        "    res = metric.compute(predictions=true_preds, references=true_labs,\n",
        "                         zero_division=0)\n",
        "    return {k.replace(\"overall_\", \"\"): v for k,v in res.items()}\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    args.model_name, num_labels=num_labels,\n",
        "    id2label=id2label, label2id=label2id)\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    output_dir     = args.output_dir,\n",
        "    eval_strategy=\"epoch\",           # ← one eval *after* each epoch\n",
        "    save_strategy  =\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    num_train_epochs=args.num_train_epochs,\n",
        "    per_device_train_batch_size=args.per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
        "    learning_rate = args.learning_rate,\n",
        "    weight_decay  = args.weight_decay,\n",
        "    warmup_steps  = args.warmup_steps,\n",
        "    seed = args.seed,\n",
        "    fp16 = torch.cuda.is_available(),\n",
        "    report_to=\"none\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=train_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset =dev_ds,\n",
        "    tokenizer=tok,\n",
        "    data_collator=DataCollatorForTokenClassification(tok),\n",
        "    compute_metrics=compute_metrics)\n",
        "\n",
        "# 8. TRAIN & FINAL EVALUATION --------------------------------------------------\n",
        "log.info(\"⏳  Starting fine‑tuning …\")\n",
        "trainer.train()\n",
        "log.info(\"✅  Finished training.  Best dev‑set F1: %.4f\",\n",
        "         trainer.state.best_metric or -1)\n",
        "\n",
        "log.info(\"🏁  Test‑set metrics:\")\n",
        "print(trainer.evaluate(eval_dataset=test_ref))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584,
          "referenced_widgets": [
            "f1c23a45faf540c1b64e6d6b50414069",
            "7ee6d20dfac3448bb08878b66ac549a2",
            "247f40186de54f4da7a6f67a6419888e",
            "85b531020fe3481da16e3c7ab6a3361e",
            "fbc311186ebb424ebe278f5655c12d94",
            "29878746180147899a5c7aaab687d3b1",
            "292d32db09e949be9c01e43cc9e6eef8",
            "6da0b38df6364e84983180fae9f0fa09",
            "38cfb69eeaf84cd39780a5f232087e20",
            "f8e0ceab96c44b328f9621ee4a0c2482",
            "bbeea462e5664dc7b40dcdc9c6bcdda6"
          ]
        },
        "id": "OCec7raq-h_-",
        "outputId": "09f7c611-ee33-4ac7-c99b-9af9d869fbbe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2567 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1c23a45faf540c1b64e6d6b50414069"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-7-cc99a0954ec2>:192: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1205' max='1205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1205/1205 02:59, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Loc</th>\n",
              "      <th>Org</th>\n",
              "      <th>Per</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.390798</td>\n",
              "      <td>{'precision': 0.9141859544292862, 'recall': 0.9885563037639641, 'f1': 0.9499177139437462, 'number': 25691}</td>\n",
              "      <td>{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 523}</td>\n",
              "      <td>{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 704}</td>\n",
              "      <td>0.914186</td>\n",
              "      <td>0.943495</td>\n",
              "      <td>0.928609</td>\n",
              "      <td>0.917318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.320645</td>\n",
              "      <td>{'precision': 0.9175696312995418, 'recall': 0.9822505935930871, 'f1': 0.9488090538228714, 'number': 25691}</td>\n",
              "      <td>{'precision': 0.23076923076923078, 'recall': 0.0057361376673040155, 'f1': 0.011194029850746268, 'number': 523}</td>\n",
              "      <td>{'precision': 0.9090909090909091, 'recall': 0.014204545454545454, 'f1': 0.027972027972027972, 'number': 704}</td>\n",
              "      <td>0.917242</td>\n",
              "      <td>0.937960</td>\n",
              "      <td>0.927485</td>\n",
              "      <td>0.919045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.544400</td>\n",
              "      <td>0.267844</td>\n",
              "      <td>{'precision': 0.9242165034761403, 'recall': 0.9883227589428204, 'f1': 0.9551952449025657, 'number': 25691}</td>\n",
              "      <td>{'precision': 0.48, 'recall': 0.045889101338432124, 'f1': 0.08376963350785341, 'number': 523}</td>\n",
              "      <td>{'precision': 0.8434782608695652, 'recall': 0.1377840909090909, 'f1': 0.23687423687423687, 'number': 704}</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.947767</td>\n",
              "      <td>0.935259</td>\n",
              "      <td>0.926245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.544400</td>\n",
              "      <td>0.249893</td>\n",
              "      <td>{'precision': 0.951655881233346, 'recall': 0.9731034214316298, 'f1': 0.9622601566559535, 'number': 25691}</td>\n",
              "      <td>{'precision': 0.4293193717277487, 'recall': 0.3135755258126195, 'f1': 0.36243093922651937, 'number': 523}</td>\n",
              "      <td>{'precision': 0.5741029641185648, 'recall': 0.5227272727272727, 'f1': 0.5472118959107807, 'number': 704}</td>\n",
              "      <td>0.935478</td>\n",
              "      <td>0.948510</td>\n",
              "      <td>0.941949</td>\n",
              "      <td>0.933228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.236200</td>\n",
              "      <td>0.245809</td>\n",
              "      <td>{'precision': 0.9505841430677908, 'recall': 0.9786306488653614, 'f1': 0.9644035289604909, 'number': 25691}</td>\n",
              "      <td>{'precision': 0.5506756756756757, 'recall': 0.31166347992351817, 'f1': 0.398046398046398, 'number': 523}</td>\n",
              "      <td>{'precision': 0.5968503937007874, 'recall': 0.5383522727272727, 'f1': 0.5660941000746826, 'number': 704}</td>\n",
              "      <td>0.938057</td>\n",
              "      <td>0.954157</td>\n",
              "      <td>0.946039</td>\n",
              "      <td>0.937907</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.24383287131786346, 'eval_LOC': {'precision': 0.9393346379647749, 'recall': 0.96579476861167, 'f1': 0.9523809523809522, 'number': 497}, 'eval_ORG': {'precision': 0.16666666666666666, 'recall': 0.1111111111111111, 'f1': 0.13333333333333333, 'number': 9}, 'eval_PER': {'precision': 0.3333333333333333, 'recall': 0.2727272727272727, 'f1': 0.3, 'number': 11}, 'eval_precision': 0.9201520912547528, 'eval_recall': 0.9361702127659575, 'eval_f1': 0.9280920421860019, 'eval_accuracy': 0.9291044776119403, 'eval_runtime': 0.0887, 'eval_samples_per_second': 574.886, 'eval_steps_per_second': 22.545, 'epoch': 5.0}\n"
          ]
        }
      ]
    }
  ]
}
