{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3F5Ob9wA2POr"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "!pip install requests beautifulsoup4 nltk pandas tqdm scikit-learn transformers torch spacy simalign pandas numpy matplotlib\n",
        "\n",
        "!pip install indic-nlp-library\n",
        "\n",
        "!python -m spacy download xx_ent_wiki_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# FULL SCRIPT  – single‑pass crawl + multi‑threshold export\n",
        "###############################################################################\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import spacy\n",
        "import logging\n",
        "import numpy as np\n",
        "from simalign import SentenceAligner\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "from indicnlp import common\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ------------------------------------------------------------------ #\n",
        "#  Polite, retry‑aware HTTP session for the Wikimedia API\n",
        "# ------------------------------------------------------------------ #\n",
        "session = requests.Session()\n",
        "session.headers.update({\n",
        "    \"User-Agent\": \"WikiMiner/1.0 (sample@email.com)\"  # <-- put real email\n",
        "})\n",
        "retry = Retry(\n",
        "    total=5,                # up to 5 retries\n",
        "    backoff_factor=1.5,     # sleep: 1.5s, 3s, 4.5s, ...\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
        "session.mount(\"http://\",  HTTPAdapter(max_retries=retry))\n",
        "# ------------------------------------------------------------------ #\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "###############################################################################\n",
        "#                              CONFIGURATION\n",
        "###############################################################################\n",
        "\n",
        "INDIC_LANGUAGES = [\n",
        "    'as', 'bn', 'gu', 'hi', 'kn', 'ml', 'mr', 'or', 'pa', 'ta', 'te',\n",
        "]\n",
        "\n",
        "LANG_CODE_MAP = {\n",
        "    'as': 'Assamese', 'bn': 'Bengali', 'gu': 'Gujarati', 'hi': 'Hindi',\n",
        "    'kn': 'Kannada',  'ml': 'Malayalam', 'mr': 'Marathi', 'or': 'Oriya',\n",
        "    'pa': 'Punjabi',  'ta': 'Tamil',     'te': 'Telugu',\n",
        "}\n",
        "\n",
        "# Define categories to crawl in English, which (hopefully) contain articles relevant\n",
        "# to each language. You can customize or expand these categories as needed.\n",
        "language_categories = {\n",
        "    'as': [\n",
        "        \"Articles containing Assamese-language text\",\n",
        "        \"Assamese-language actors\",\n",
        "        \"People from Assam\",\n",
        "        \"Assam Valley Literary Award\",\n",
        "        \"Assamese singers\",\n",
        "        \"Novels in Assamese\",\n",
        "        \"CS1 Assamese-language sources (as)\",\n",
        "        \"Assamese-language film stubs\",\n",
        "        \"Assamese-language mass media\",\n",
        "        \"Assamese people\",\n",
        "        \"Assamese nationalism\",\n",
        "        # Newly added categories for Assamese\n",
        "        \"Assamese literature\",\n",
        "        \"Cinema of Assam\",\n",
        "        \"Music of Assam\",\n",
        "        \"Festivals in Assam\",\n",
        "        \"Assamese cuisine\",\n",
        "        \"Assamese folklore & mythology\",\n",
        "        \"Dances and theatre of Assam\",\n",
        "        \"Assamese-language media\",\n",
        "        \"Notable Assamese people\",\n",
        "        \"Awards and honors in Assam\",\n",
        "    ],\n",
        "    'bn': [\n",
        "        \"Bengali-language films\",\n",
        "        \"Bengali people\",\n",
        "        \"Bengali-language writers\",\n",
        "        \"Bengali words and phrases\",\n",
        "        \"Bengali language\",\n",
        "        \"Articles containing Bengali-language text\",\n",
        "        \"Bengali poetry in English translation\",\n",
        "        \"Songs in Bengali\",\n",
        "        \"Bengali nationalism\",\n",
        "        \"Bengali-language surnames\",\n",
        "        # Newly added categories for Bengali\n",
        "        \"Bengali literature\",\n",
        "        \"Bengali cinema\",\n",
        "        \"Bengali music and songs\",\n",
        "        \"Festivals of Bengal\",\n",
        "        \"Bengali cuisine\",\n",
        "        \"Bengali mythology & folk tales\",\n",
        "        \"Bengali theatre and dance\",\n",
        "        \"Notable Bengali people\",\n",
        "        \"Literary awards in Bengali\",\n",
        "        \"Bengali media\",\n",
        "    ],\n",
        "    'gu': [\n",
        "        \"Gujarati people\",\n",
        "        \"Gujarati writers\",\n",
        "        \"Articles containing Gujarati-language text\",\n",
        "        \"Gujarati literature\",\n",
        "        \"Gujarati-language films\",\n",
        "        \"Gujarati-language books\",\n",
        "        \"Gujarati-language writers\",\n",
        "        \"Gujarati language\",\n",
        "        \"Articles with Gujarati-language sources (gu)\",\n",
        "        \"Gujarati-language songs\",\n",
        "        # Newly added categories for Gujarati\n",
        "        \"Gujarati cinema\",\n",
        "        \"Gujarati music\",\n",
        "        \"Festivals in Gujarat\",\n",
        "        \"Gujarati cuisine\",\n",
        "        \"Gujarati folklore & mythology\",\n",
        "        \"Gujarati theatre (Bhavai)\",\n",
        "        \"Gujarati media\",\n",
        "        \"Notable Gujarati people\",\n",
        "        \"Cultural institutions & awards\",\n",
        "    ],\n",
        "    'hi': [\n",
        "        \"Hindi\",\n",
        "        \"Hindi-language films\",\n",
        "        \"Actors in Hindi cinema\",\n",
        "        \"Hindi-language literature\",\n",
        "        \"Hindi-language film directors\",\n",
        "        \"Hindi-language writers\",\n",
        "        \"Articles containing Hindi-language text\",\n",
        "        \"Hindi theatre\",\n",
        "        \"Hindi-language poets\",\n",
        "        # Newly added categories for Hindi\n",
        "        \"Hindi literature\",\n",
        "        \"Hindi cinema (Bollywood)\",\n",
        "        \"Hindi music and songs\",\n",
        "        \"Indian festivals (North India)\",\n",
        "        \"Hindu epics and mythology\",\n",
        "        \"Hindi media\",\n",
        "        \"Notable Hindi authors and poets\",\n",
        "        \"Hindi theatre and folk arts\",\n",
        "        \"National awards (Hindi context)\",\n",
        "        \"Historical events (Hindi belt)\",\n",
        "    ],\n",
        "    'kn': [\n",
        "        \"Kannada-language films\",\n",
        "        \"Kannada poets\",\n",
        "        \"Kannada playback singers\",\n",
        "        \"Kannada film directors\",\n",
        "        \"Kannada literature\",\n",
        "        \"Kannada cinema\",\n",
        "        \"Kannada grammar\",\n",
        "        \"Kannada-language writers\",\n",
        "        \"Articles containing Kannada-language text\",\n",
        "        \"Kannada people\",\n",
        "        # Newly added categories for Kannada\n",
        "        \"Kannada music\",\n",
        "        \"Festivals of Karnataka\",\n",
        "        \"Kannada cuisine\",\n",
        "        \"Kannada folklore & mythology\",\n",
        "        \"Kannada theater and dance\",\n",
        "        \"Notable Kannada people\",\n",
        "        \"Awards in Kannada\",\n",
        "        \"Kannada media\",\n",
        "    ],\n",
        "    'ml': [\n",
        "        \"Malayalam-language films\",\n",
        "        \"Malayalam cinema\",\n",
        "        \"Articles containing Malayalam-language text\",\n",
        "        \"Malayalam poets\",\n",
        "        \"Malayalam-language writers\",\n",
        "        \"Malayalam grammar\",\n",
        "        \"Malayalam-language newspapers\",\n",
        "        \"Malayalam language\",\n",
        "        \"Malayalam-language novelists\",\n",
        "        # Newly added categories for Malayalam\n",
        "        \"Malayalam literature\",\n",
        "        \"Malayalam music\",\n",
        "        \"Festivals of Kerala\",\n",
        "        \"Malayalam cuisine\",\n",
        "        \"Kerala folklore & mythology\",\n",
        "        \"Malayalam theatre and dance\",\n",
        "        \"Notable Malayalam people\",\n",
        "        \"Malayalam media\",\n",
        "        \"Kerala history events\",\n",
        "    ],\n",
        "    'mr': [\n",
        "        \"Marathi language\",\n",
        "        \"Marathi people\",\n",
        "        \"Marathi-language films\",\n",
        "        \"Actors in Marathi theatre\",\n",
        "        \"Marathi-language writers\",\n",
        "        \"Articles containing Marathi-language text\",\n",
        "        \"Marathi terms\",\n",
        "        \"Marathi-language literature\",\n",
        "        \"Marathi-language poets\",\n",
        "        # Newly added categories for Marathi\n",
        "        \"Marathi literature\",\n",
        "        \"Marathi cinema\",\n",
        "        \"Marathi music\",\n",
        "        \"Festivals in Maharashtra\",\n",
        "        \"Marathi cuisine\",\n",
        "        \"Marathi mythology & Bhakti saints\",\n",
        "        \"Marathi theatre (Sangeet Natak)\",\n",
        "        \"Notable Marathi people\",\n",
        "        \"Marathi media\",\n",
        "        \"Awards in Marathi\",\n",
        "    ],\n",
        "    'or': [\n",
        "        \"Odia language\",\n",
        "        \"Articles containing Odia-language text\",\n",
        "        \"Odia-language writers\",\n",
        "        \"Odia cuisine\",\n",
        "        \"Actors in Odia cinema\",\n",
        "        \"Odia literature\",\n",
        "        \"Odia people\",\n",
        "        \"Odia-language films\",\n",
        "        \"Odia-language actors\",\n",
        "        \"Odia Hindu saints\",\n",
        "        # Newly added categories for Odia\n",
        "        \"Odia music\",\n",
        "        \"Festivals of Odisha\",\n",
        "        \"Odisha folklore & mythology\",\n",
        "        \"Odissi dance and arts\",\n",
        "        \"Notable Odia people\",\n",
        "        \"Odia media\",\n",
        "        \"Odisha awards and organizations\",\n",
        "    ],\n",
        "    'pa': [\n",
        "        \"Punjabi festivals\",\n",
        "        \"Punjabi language\",\n",
        "        \"Punjabi people\",\n",
        "        \"Punjabi dialects\",\n",
        "        \"Punjabi words and phrases\",\n",
        "        \"Punjabi music\",\n",
        "        \"Punjabi-language films\",\n",
        "        \"Punjabi-language writers\",\n",
        "        \"Punjabi-language singers\",\n",
        "        \"Articles containing Punjabi-language text\",\n",
        "        \"Punjabi-language surnames\",\n",
        "        # Newly added categories for Punjabi\n",
        "        \"Punjabi literature\",\n",
        "        \"Punjabi cinema\",\n",
        "        \"Festivals of Punjab\",\n",
        "        \"Punjabi cuisine\",\n",
        "        \"Sikh mythology and history\",\n",
        "        \"Punjabi folk culture\",\n",
        "        \"Notable Punjabi people\",\n",
        "        \"Punjabi media\",\n",
        "        \"Punjab region history events\",\n",
        "    ],\n",
        "    'ta': [\n",
        "        \"Tamil\",\n",
        "        \"Tamil people\",\n",
        "        \"Tamil-language films\",\n",
        "        \"Tamil film directors\",\n",
        "        \"Tamil diaspora\",\n",
        "        \"Tamil film producers\",\n",
        "        \"Tamil writers\",\n",
        "        \"Articles containing Tamil-language text\",\n",
        "        \"Tamil architecture\",\n",
        "        \"People from Tamil Nadu\",\n",
        "        \"Tamil scholars\",\n",
        "        # Newly added categories for Tamil\n",
        "        \"Tamil literature\",\n",
        "        \"Tamil cinema\",\n",
        "        \"Tamil music and songs\",\n",
        "        \"Tamil festivals\",\n",
        "        \"Tamil cuisine\",\n",
        "        \"Tamil mythology and religion\",\n",
        "        \"Tamil dynasties and history\",\n",
        "        \"Notable Tamil people\",\n",
        "        \"Tamil media\",\n",
        "        \"Tamil theatre and arts\",\n",
        "    ],\n",
        "    'te': [\n",
        "        \"Telugu people\",\n",
        "        \"Telugu-language films\",\n",
        "        \"Telugu actors\",\n",
        "        \"Telugu cinema\",\n",
        "        \"Telugu film directors\",\n",
        "        \"Telugu language\",\n",
        "        \"Articles containing Telugu-language text\",\n",
        "        \"Telugu writers\",\n",
        "        \"Telugu poets\",\n",
        "        \"Telugu names\",\n",
        "        # Newly added categories for Telugu\n",
        "        \"Telugu literature\",\n",
        "        \"Telugu music\",\n",
        "        \"Festivals in Andhra/Telangana\",\n",
        "        \"Telugu cuisine\",\n",
        "        \"Telugu mythology & religion\",\n",
        "        \"Telugu theater\",\n",
        "        \"Notable Telugu people\",\n",
        "        \"Telugu media\",\n",
        "        \"Awards in Telugu\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Path to the root directory of the Indic NLP resources\n",
        "# Make sure to point this to your local \"indic_nlp_resources\" folder\n",
        "INDIC_RESOURCES_PATH = 'D:/indic_nlp_resources-master/indic_nlp_resources-master'\n",
        "common.set_resources_path(INDIC_RESOURCES_PATH)\n",
        "# /home/user/112101054_Yash/indic_nlp_resources-master  -- on ubuntu\n",
        "\n",
        "SIM_THRESHOLDS   = [0.6, 0.7, 0.8]            # τ  – cosine similarity\n",
        "ALIGN_THRESHOLDS = [0.4, 0.5, 0.6, 0.7, 0.8]  # α  – word‑alignment ratio\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#                              LOAD MODELS\n",
        "###############################################################################\n",
        "\n",
        "logging.info(\"Loading multilingual transformer model (LaBSE)...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/LaBSE')\n",
        "model     = AutoModel.from_pretrained('sentence-transformers/LaBSE')\n",
        "model.eval()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "logging.info(f\"Transformer model loaded on device: {device}\")\n",
        "\n",
        "logging.info(\"Loading SpaCy models...\")\n",
        "try:\n",
        "    nlp_en = spacy.load('en_core_web_sm')\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error loading SpaCy English model: {e}\")\n",
        "\n",
        "if \"sentencizer\" not in nlp_en.pipe_names:\n",
        "    logging.info(\"Adding 'sentencizer' to SpaCy English pipeline.\")\n",
        "    nlp_en.add_pipe(\"sentencizer\", first=True)\n",
        "\n",
        "aligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods=\"mai\")\n",
        "\n",
        "###############################################################################\n",
        "#                              HELPER FUNCTIONS\n",
        "###############################################################################\n",
        "\n",
        "def get_articles_in_category(category_title, lang):\n",
        "    \"\"\"\n",
        "    Fetches all article titles under a Wikipedia category.\n",
        "    \"\"\"\n",
        "    url = f'https://{lang}.wikipedia.org/w/api.php'\n",
        "    params = {\n",
        "        'action': 'query',\n",
        "        'list': 'categorymembers',\n",
        "        'cmtitle': f'Category:{category_title}',\n",
        "        'cmlimit': 'max',\n",
        "        'format': 'json',\n",
        "    }\n",
        "    articles = []\n",
        "    while True:\n",
        "        try:\n",
        "            response = session.get(url, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            for page in data['query']['categorymembers']:\n",
        "                if page['ns'] == 0:  # ns=0 indicates articles (not subcategories or files)\n",
        "                    articles.append(page['title'])\n",
        "            if 'continue' in data:\n",
        "                params['cmcontinue'] = data['continue']['cmcontinue']\n",
        "            else:\n",
        "                break\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logging.error(f\"Request failed for category '{category_title}' in '{lang}': {e}\")\n",
        "            break\n",
        "    return articles\n",
        "\n",
        "def get_langlinks(title, source_lang, target_lang):\n",
        "    \"\"\"\n",
        "    Checks if an article in source_lang has an equivalent article in target_lang.\n",
        "    Returns the title in target_lang if exists, else None.\n",
        "    \"\"\"\n",
        "    url = f'https://{source_lang}.wikipedia.org/w/api.php'\n",
        "    params = {\n",
        "        'action': 'query',\n",
        "        'titles': title,\n",
        "        'prop': 'langlinks',\n",
        "        'lllang': target_lang,\n",
        "        'format': 'json',\n",
        "    }\n",
        "    try:\n",
        "        response = session.get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        pages = data['query']['pages']\n",
        "        for page_id in pages:\n",
        "            if 'langlinks' in pages[page_id]:\n",
        "                return pages[page_id]['langlinks'][0]['*']\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Request failed for langlinks of page '{title}' from '{source_lang}' to '{target_lang}': {e}\")\n",
        "    return None\n",
        "\n",
        "def get_wikipedia_page(title, lang):\n",
        "    \"\"\"\n",
        "    Fetches the Wikipedia page content for a given title and language.\n",
        "    \"\"\"\n",
        "    url = f'https://{lang}.wikipedia.org/w/api.php'\n",
        "    params = {\n",
        "        'action': 'parse',\n",
        "        'page': title,\n",
        "        'prop': 'text',\n",
        "        'format': 'json',\n",
        "        'redirects': True,\n",
        "    }\n",
        "    try:\n",
        "        response = session.get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if 'error' in data:\n",
        "            logging.warning(f\"Error fetching page '{title}' in '{lang}': {data['error']['info']}\")\n",
        "            return None\n",
        "        html_content = data['parse']['text']['*']\n",
        "        return html_content\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Request failed for page '{title}' in '{lang}': {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_text(html_content):\n",
        "    \"\"\"\n",
        "    Extracts and cleans text from HTML content.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Remove certain tags which are not part of main text content\n",
        "    for element in soup(['table', 'sup', 'span', 'ol', 'ul', 'style', 'script']):\n",
        "        element.decompose()\n",
        "\n",
        "    text = soup.get_text(separator=' ')\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def split_into_sentences(text, lang):\n",
        "    \"\"\"\n",
        "    Splits text into sentences using appropriate tokenizer based on language.\n",
        "    \"\"\"\n",
        "    if lang == 'en':\n",
        "        doc = nlp_en(text)\n",
        "        sentences = [sent.text.strip() for sent in doc.sents]\n",
        "    else:\n",
        "        # For Indic languages, use the Indic NLP library\n",
        "        sentences = sentence_tokenize.sentence_split(text, lang)\n",
        "    return sentences\n",
        "\n",
        "def encode_sentences(sentences):\n",
        "    \"\"\"\n",
        "    Encodes sentences into vectors using a multilingual transformer model.\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    batch_size = 32\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(sentences), batch_size):\n",
        "            batch = sentences[i:i+batch_size]\n",
        "            inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "            outputs = model(**inputs)\n",
        "            batch_embeddings = outputs.pooler_output.cpu()\n",
        "            embeddings.append(batch_embeddings)\n",
        "    if embeddings:\n",
        "        embeddings = torch.cat(embeddings, dim=0)\n",
        "    else:\n",
        "        embeddings = torch.tensor([])\n",
        "    return embeddings\n",
        "\n",
        "def compute_similarity(embeddings_en, embeddings_target):\n",
        "    \"\"\"\n",
        "    Computes cosine similarity between English and target language sentence embeddings.\n",
        "    \"\"\"\n",
        "    if embeddings_en.size(0) == 0 or embeddings_target.size(0) == 0:\n",
        "        logging.warning(\"One of the embedding sets is empty. Cannot compute similarity.\")\n",
        "        return None\n",
        "\n",
        "    embeddings_en_np = embeddings_en.numpy()\n",
        "    embeddings_target_np = embeddings_target.numpy()\n",
        "\n",
        "    # Normalize each embedding vector\n",
        "    embeddings_en_norm = embeddings_en_np / np.linalg.norm(embeddings_en_np, axis=1, keepdims=True)\n",
        "    embeddings_target_norm = embeddings_target_np / np.linalg.norm(embeddings_target_np, axis=1, keepdims=True)\n",
        "\n",
        "    sim_matrix = cosine_similarity(embeddings_en_norm, embeddings_target_norm)\n",
        "    return sim_matrix\n",
        "\n",
        "def align_sentences(sentences_en, sentences_target, sim_matrix, threshold=0.8, alignment_threshold=0.8):\n",
        "    \"\"\"\n",
        "    Aligns sentences based on similarity scores with bidirectional best matching and filtering.\n",
        "    Uses word alignment from SimAlign to check alignment quality.\n",
        "\n",
        "    :param sentences_en: list of English sentences\n",
        "    :param sentences_target: list of target-language sentences\n",
        "    :param sim_matrix: computed similarity matrix\n",
        "    :param threshold: minimum similarity score for alignment\n",
        "    :param alignment_threshold: proportion of aligned words required to confirm alignment\n",
        "    :return: list of (en_sentence, tgt_sentence, alignment_score, sim_score)\n",
        "    \"\"\"\n",
        "    aligned_sentences = []\n",
        "\n",
        "    # For each English sentence, find the best target match\n",
        "    best_matches_en_to_tgt = sim_matrix.argmax(axis=1)\n",
        "    best_scores_en_to_tgt = sim_matrix.max(axis=1)\n",
        "\n",
        "    # For each target sentence, find the best English match\n",
        "    best_matches_tgt_to_en = sim_matrix.argmax(axis=0)\n",
        "    best_scores_tgt_to_en = sim_matrix.max(axis=0)\n",
        "\n",
        "    for en_idx, tgt_idx in enumerate(best_matches_en_to_tgt):\n",
        "        sim_score = best_scores_en_to_tgt[en_idx]\n",
        "        # Check if the similarity is above threshold\n",
        "        if sim_score < threshold:\n",
        "            continue\n",
        "\n",
        "        # Check if it's a mutual best match\n",
        "        if best_matches_tgt_to_en[tgt_idx] == en_idx and best_scores_tgt_to_en[tgt_idx] >= threshold:\n",
        "            en_sentence = sentences_en[en_idx]\n",
        "            tgt_sentence = sentences_target[tgt_idx]\n",
        "\n",
        "            # Length filtering\n",
        "            len_en = len(en_sentence.split())\n",
        "            len_tgt = len(tgt_sentence.split())\n",
        "            if min(len_en, len_tgt) == 0:\n",
        "                continue\n",
        "            length_ratio = max(len_en, len_tgt) / min(len_en, len_tgt)\n",
        "            if length_ratio > 1.3:\n",
        "                continue\n",
        "\n",
        "            # Word alignment using SimAlign\n",
        "            alignments = aligner.get_word_aligns(en_sentence, tgt_sentence)\n",
        "            num_aligned_words = len(alignments['inter'])\n",
        "            alignment_score = num_aligned_words / min(len_en, len_tgt)\n",
        "\n",
        "            if alignment_score >= alignment_threshold:\n",
        "                aligned_sentences.append((en_sentence, tgt_sentence, alignment_score, float(sim_score)))\n",
        "\n",
        "    return aligned_sentences\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#                              MAIN LOGIC\n",
        "###############################################################################\n",
        "\n",
        "def main(\n",
        "    langs: list[str] | None = None,\n",
        "    sim_thresholds: list[float] | None = None,\n",
        "    alpha_thresholds: list[float] | None = None,\n",
        "):\n",
        "    # reslove parameter overrides ------------------------------\n",
        "    if langs is None:\n",
        "        langs = INDIC_LANGUAGES\n",
        "    if sim_thresholds is None:\n",
        "        sim_thresholds = SIM_THRESHOLDS\n",
        "    if alpha_thresholds is None:\n",
        "        alpha_thresholds = ALIGN_THRESHOLDS\n",
        "\n",
        "    EN_LANG = 'en'\n",
        "\n",
        "    for lang_code in langs:\n",
        "        language_name = LANG_CODE_MAP.get(lang_code, lang_code)\n",
        "        logging.info(f\"==================== {language_name} ({lang_code}) \"\n",
        "                     \"====================\")\n",
        "\n",
        "        #######################################################################\n",
        "        # Containers for one single crawl pass\n",
        "        #######################################################################\n",
        "        all_pairs         = []      # every (sentence, sim, align, category)\n",
        "        article_category  = {}      # article_title -> first English category\n",
        "        #######################################################################\n",
        "\n",
        "        # ------------- 1. COLLECT ARTICLE TITLES (unchanged + mapping) ------\n",
        "        categories = language_categories.get(lang_code, [])\n",
        "        if not categories:\n",
        "            logging.warning(f\"No categories specified for {language_name}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        article_titles = set()\n",
        "        for cat in categories:\n",
        "            titles_en = get_articles_in_category(cat, EN_LANG)\n",
        "            for t in titles_en:\n",
        "                article_category.setdefault(t, cat)   # store only first cat\n",
        "            article_titles.update(titles_en)\n",
        "\n",
        "        total_articles_fetched = len(article_titles)\n",
        "        logging.info(f\"Found {total_articles_fetched} unique English articles.\")\n",
        "\n",
        "        # ------ 2. PROCESS EACH BILINGUAL ARTICLE (download & align once) ----\n",
        "        titles_with_langlinks = 0\n",
        "        for title in tqdm(article_titles,\n",
        "                          desc=f\"Langlinks > {language_name}\",\n",
        "                          leave=False):\n",
        "            tgt_title = get_langlinks(title, EN_LANG, lang_code)\n",
        "            if tgt_title is None:\n",
        "                continue\n",
        "            titles_with_langlinks += 1\n",
        "\n",
        "            html_en  = get_wikipedia_page(title,     EN_LANG)\n",
        "            html_tgt = get_wikipedia_page(tgt_title, lang_code)\n",
        "            if not html_en or not html_tgt:\n",
        "                continue\n",
        "\n",
        "            text_en  = extract_text(html_en)\n",
        "            text_tgt = extract_text(html_tgt)\n",
        "            if not text_en or not text_tgt:\n",
        "                continue\n",
        "\n",
        "            # ---- sentence split & length filter ----------------------------\n",
        "            def filter_sents(sents, lo=5, hi=200):\n",
        "                return [s for s in sents if lo <= len(s.split()) <= hi]\n",
        "\n",
        "            sents_en  = filter_sents(split_into_sentences(text_en,  EN_LANG))\n",
        "            sents_tgt = filter_sents(split_into_sentences(text_tgt, lang_code))\n",
        "            if not sents_en or not sents_tgt:\n",
        "                continue\n",
        "\n",
        "            # ---- embed once -------------------------------------------------\n",
        "            emb_en  = encode_sentences(sents_en)\n",
        "            emb_tgt = encode_sentences(sents_tgt)\n",
        "            sim_mtx = compute_similarity(emb_en, emb_tgt)\n",
        "            if sim_mtx is None:\n",
        "                continue\n",
        "\n",
        "            # ---- capture all potential pairs (τ=0, α=0) ----------------\n",
        "            aligned_raw = align_sentences(\n",
        "                sents_en, sents_tgt, sim_mtx,\n",
        "                threshold=0.4, alignment_threshold=0.0\n",
        "            )\n",
        "\n",
        "            cat = article_category[title]\n",
        "            for (sen_en, sen_tgt, align_sc, sim_sc) in aligned_raw:\n",
        "                all_pairs.append({\n",
        "                    \"en\": sen_en,\n",
        "                    \"tgt\": sen_tgt,\n",
        "                    \"sim\": sim_sc,\n",
        "                    \"align\": align_sc,\n",
        "                    \"category\": cat\n",
        "                })\n",
        "        logging.info(f\"{titles_with_langlinks}/{total_articles_fetched} \"\n",
        "                     f\"articles had {language_name} counterparts.\")\n",
        "        logging.info(f\"Total raw sentence pairs harvested: {len(all_pairs)}\")\n",
        "\n",
        "        # ---------- 3. MULTI‑THRESHOLD OUTPUT LOOP --------------------------\n",
        "        for tau in sim_thresholds:\n",
        "            for alpha in alpha_thresholds:\n",
        "                # Directory structure:  parallel_datasets/<τ>/<α>/…\n",
        "                ds_dir = os.path.join(\"parallel_datasets\", str(tau), str(alpha))\n",
        "                st_dir = os.path.join(\"stats\",             str(tau), str(alpha))\n",
        "                os.makedirs(ds_dir, exist_ok=True)\n",
        "                os.makedirs(st_dir, exist_ok=True)\n",
        "\n",
        "                # Filter pairs that satisfy the current grid point\n",
        "                selected = [\n",
        "                    p for p in all_pairs\n",
        "                    if p[\"sim\"] >= tau and p[\"align\"] >= alpha\n",
        "                ]\n",
        "\n",
        "                # ---------- PARALLEL DATASET JSONL --------------------------\n",
        "                df = pd.DataFrame(\n",
        "                    [(p[\"en\"], p[\"tgt\"]) for p in selected],\n",
        "                    columns=[\"English\", language_name]\n",
        "                )\n",
        "                ds_path = os.path.join(\n",
        "                    ds_dir, f\"parallel_dataset_{language_name.lower()}.json\"\n",
        "                )\n",
        "                df.to_json(ds_path, orient=\"records\",\n",
        "                           lines=True, force_ascii=False)\n",
        "\n",
        "                # ---------- STATS (aggregate + per‑category) ---------------\n",
        "                agg = {\n",
        "                    \"language\": language_name,\n",
        "                    \"tau\": tau,\n",
        "                    \"alpha\": alpha,\n",
        "                    \"num_pairs\": len(selected),\n",
        "                    \"mean_cosine\": (float(np.mean([p[\"sim\"]\n",
        "                                      for p in selected]))\n",
        "                                    if selected else 0.0),\n",
        "                    \"mean_alignment\": (float(np.mean([p[\"align\"]\n",
        "                                         for p in selected]))\n",
        "                                       if selected else 0.0)\n",
        "                }\n",
        "                by_cat = {}\n",
        "                for cat in categories:\n",
        "                    cp = [p for p in selected if p[\"category\"] == cat]\n",
        "                    if cp:\n",
        "                        by_cat[cat] = {\n",
        "                            \"num_pairs\": len(cp),\n",
        "                            \"mean_cosine\": float(np.mean([p[\"sim\"] for p in cp])),\n",
        "                            \"mean_alignment\": float(np.mean([p[\"align\"]\n",
        "                                                             for p in cp]))\n",
        "                        }\n",
        "\n",
        "                st_path = os.path.join(\n",
        "                    st_dir, f\"stats_{language_name.lower()}.json\"\n",
        "                )\n",
        "                with open(st_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump({\"aggregate\": agg, \"by_category\": by_cat},\n",
        "                              f, ensure_ascii=False, indent=4)\n",
        "\n",
        "                logging.info(f\"[{language_name}] τ={tau} α={alpha} \"\n",
        "                             f\"→ {len(selected)} pairs \"\n",
        "                             f\"(saved: {ds_path})\")\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "if __name__ == \"__main__\":\n",
        "    #default execution\n",
        "    main()\n",
        "\n",
        "    # run just Hindi with one grid point\n",
        "    # main(\n",
        "    #     langs=['hi'],\n",
        "    #     sim_thresholds=[0.7],    # τ\n",
        "    #     alpha_thresholds=[0.6]   # α\n",
        "    # )\n",
        "\n",
        "    # # run Bengali + Gujarati, full default grids\n",
        "    # main(langs=['bn', 'gu'])\n",
        "\n",
        "    # # run everything but with a finer τ grid\n",
        "    # main(sim_thresholds=[0.55, 0.6, 0.7, 0.8])\n",
        "\n",
        "\n",
        "    # # Should run one small grid without errors\n",
        "    # main(langs=['as'], sim_thresholds=[0.7], alpha_thresholds=[0.6])\n"
      ],
      "metadata": {
        "id": "8RPpeakc2QJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1.  Imports & setup\n",
        "# ------------------------------------------------------------\n",
        "import json, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "STATS_ROOT = \"stats\"\n",
        "TAU_GRID   = [0.6, 0.7, 0.8]\n",
        "ALPHA_GRID = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "LANGS      = [\"as\",\"bn\",\"gu\",\"hi\",\"kn\",\"ml\",\"mr\",\"or\",\"pa\",\"ta\",\"te\"]\n",
        "LANG_FULL = {\n",
        "    'as':'Assamese','bn':'Bengali','gu':'Gujarati','hi':'Hindi',\n",
        "    'kn':'Kannada','ml':'Malayalam','mr':'Marathi','or':'Oriya',\n",
        "    'pa':'Punjabi','ta':'Tamil','te':'Telugu'\n",
        "}\n",
        "\n",
        "def load_stats(lang, tau, alpha):\n",
        "    path = os.path.join(STATS_ROOT, str(tau), str(alpha),\n",
        "                        f\"stats_{LANG_FULL[lang].lower()}.json\")\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2.  Sweet‑spot heat‑map (robust to missing files)\n",
        "# ------------------------------------------------------------\n",
        "fig, axes = plt.subplots(4, 3, figsize=(15,20))\n",
        "axes = axes.flatten()\n",
        "cmap = plt.get_cmap('plasma')\n",
        "\n",
        "for idx, lang in enumerate(LANGS):\n",
        "    grid_cos = np.zeros((len(ALPHA_GRID), len(TAU_GRID)))\n",
        "    grid_cnt = np.zeros_like(grid_cos, dtype=int)\n",
        "\n",
        "    for i, alpha in enumerate(ALPHA_GRID):\n",
        "        for j, tau in enumerate(TAU_GRID):\n",
        "            stats = load_stats(lang, tau, alpha)\n",
        "            if stats:\n",
        "                grid_cos[i,j] = stats[\"aggregate\"][\"mean_cosine\"]\n",
        "                grid_cnt[i,j] = stats[\"aggregate\"][\"num_pairs\"]\n",
        "            else:\n",
        "                grid_cos[i,j] = np.nan\n",
        "                grid_cnt[i,j] = 0\n",
        "\n",
        "    ax = axes[idx]\n",
        "    im = ax.imshow(grid_cos, aspect=\"auto\", cmap=cmap, vmin=0.5, vmax=1.0)\n",
        "    ax.set_xticks(range(len(TAU_GRID))); ax.set_xticklabels(TAU_GRID, rotation=45)\n",
        "    ax.set_yticks(range(len(ALPHA_GRID))); ax.set_yticklabels(ALPHA_GRID)\n",
        "    ax.set_title(LANG_FULL[lang], pad=10)\n",
        "\n",
        "    for i in range(len(ALPHA_GRID)):\n",
        "        for j in range(len(TAU_GRID)):\n",
        "            val = grid_cos[i,j]\n",
        "            count = grid_cnt[i,j]\n",
        "            if np.isnan(val):\n",
        "                txt = \"–\\n0\"\n",
        "                color = \"grey\"\n",
        "            else:\n",
        "                txt = f\"{val:.2f}\\n{count}\"\n",
        "                color = 'white' if val<0.75 else 'black'\n",
        "            ax.text(j, i, txt, ha=\"center\", va=\"center\", fontsize=8, color=color)\n",
        "\n",
        "fig.subplots_adjust(right=0.85, top=0.92)\n",
        "cbar_ax = fig.add_axes([0.88,0.15,0.02,0.7])\n",
        "fig.colorbar(im, cax=cbar_ax, label='Mean Cosine')\n",
        "fig.suptitle(\"Sweet‑spot Heatmap\", fontsize=16)\n",
        "plt.tight_layout(rect=[0,0,0.85,0.96])\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3.  Category bar‑chart for τ=0.7, α=0.6\n",
        "# ------------------------------------------------------------\n",
        "SAMPLE_TAU, SAMPLE_ALPHA = 0.7, 0.6\n",
        "for lang in LANGS:\n",
        "    stats = load_stats(lang, SAMPLE_TAU, SAMPLE_ALPHA)\n",
        "    if not stats or not stats.get(\"by_category\"):\n",
        "        continue\n",
        "    df_cat = pd.DataFrame.from_dict(stats[\"by_category\"], orient=\"index\")\n",
        "    df_cat[\"num_pairs\"] = df_cat.get(\"num_pairs\", 0)\n",
        "    df_cat = df_cat.sort_values(\"num_pairs\", ascending=False).head(5)\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.barh(df_cat.index[::-1], df_cat[\"num_pairs\"][::-1], edgecolor='k')\n",
        "    plt.title(f\"Top‑5 Categories: {LANG_FULL[lang]} (τ={SAMPLE_TAU}, α={SAMPLE_ALPHA})\", pad=8)\n",
        "    plt.xlabel(\"Sentence Pairs\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4.  Executive table for τ=0.7, α=0.6\n",
        "# ------------------------------------------------------------\n",
        "rows = []\n",
        "for lang in LANGS:\n",
        "    stats = load_stats(lang, SAMPLE_TAU, SAMPLE_ALPHA)\n",
        "    if not stats:\n",
        "        rows.append({\"Language\": LANG_FULL[lang], \"Pairs\":0, \"Mean Cos\":0, \"Mean Align\":0})\n",
        "    else:\n",
        "        agg = stats[\"aggregate\"]\n",
        "        rows.append({\n",
        "            \"Language\": LANG_FULL[lang],\n",
        "            \"Pairs\":     agg[\"num_pairs\"],\n",
        "            \"Mean Cos\":  round(agg[\"mean_cosine\"],3),\n",
        "            \"Mean Align\":round(agg[\"mean_alignment\"],3)\n",
        "        })\n",
        "exec_df = pd.DataFrame(rows).sort_values(\"Pairs\", ascending=False).reset_index(drop=True)\n",
        "display(exec_df)        # Jupyter/Colab will render as a neat table\n",
        "\n",
        "# also save as CSV if you like\n",
        "exec_df.to_csv(\"executive_table_0.6_0.7.csv\", index=False)"
      ],
      "metadata": {
        "id": "dmiqTJwy7CQX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
